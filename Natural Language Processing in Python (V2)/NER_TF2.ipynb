{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4KSKIpGDmbx"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# conll 2003\n",
        "!wget -nc https://lazyprogrammer.me/course_files/nlp/ner_train.pkl\n",
        "!wget -nc https://lazyprogrammer.me/course_files/nlp/ner_test.pkl"
      ],
      "metadata": {
        "id": "uPrKMWUlGPm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ner_train.pkl', 'rb') as f:\n",
        "  corpus_train = pickle.load(f)\n",
        "\n",
        "with open('ner_test.pkl', 'rb') as f:\n",
        "  corpus_test = pickle.load(f)"
      ],
      "metadata": {
        "id": "YLQMAOVaGRPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_inputs = []\n",
        "train_targets = []\n",
        "\n",
        "for sentence_tag_pairs in corpus_train:\n",
        "  tokens = []\n",
        "  target = []\n",
        "  for token, tag in sentence_tag_pairs:\n",
        "    tokens.append(token)\n",
        "    target.append(tag)\n",
        "  train_inputs.append(tokens)\n",
        "  train_targets.append(target)"
      ],
      "metadata": {
        "id": "MJW4qcDVGS_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = []\n",
        "test_targets = []\n",
        "\n",
        "for sentence_tag_pairs in corpus_test:\n",
        "  tokens = []\n",
        "  target = []\n",
        "  for token, tag in sentence_tag_pairs:\n",
        "    tokens.append(token)\n",
        "    target.append(tag)\n",
        "  test_inputs.append(tokens)\n",
        "  test_targets.append(target)"
      ],
      "metadata": {
        "id": "OAAkSGOhGUD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, Bidirectional\n",
        "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ],
      "metadata": {
        "id": "j6uVtlJhGglP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentences to sequences\n",
        "\n",
        "MAX_VOCAB_SIZE = None\n",
        "\n",
        "# capitalization might be useful - test it\n",
        "should_lowercase = False\n",
        "word_tokenizer = Tokenizer(\n",
        "    num_words=MAX_VOCAB_SIZE,\n",
        "    lower=should_lowercase,\n",
        "    oov_token='UNK',\n",
        ")\n",
        "# otherwise unknown tokens will be removed and len(input) != len(target)\n",
        "# input words and target words will not be aligned!\n",
        "\n",
        "# it's ok to \"fit\" on the whole corpus - it just means some embeddings\n",
        "# won't be trained\n",
        "# this is because for the test set, any unknown tokens will be removed\n",
        "# which will change the length of the input (***CHECK)\n",
        "\n",
        "word_tokenizer.fit_on_texts(train_inputs)\n",
        "train_inputs_int = word_tokenizer.texts_to_sequences(train_inputs)\n",
        "test_inputs_int = word_tokenizer.texts_to_sequences(test_inputs)"
      ],
      "metadata": {
        "id": "r3syncTlGh-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get word -> integer mapping\n",
        "word2idx = word_tokenizer.word_index\n",
        "V = len(word2idx)\n",
        "print('Found %s unique tokens.' % V)"
      ],
      "metadata": {
        "id": "cmsJfsRyGjTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/11264684/flatten-list-of-lists\n",
        "def flatten(list_of_lists):\n",
        "  flattened = [val for sublist in list_of_lists for val in sublist]\n",
        "  return flattened"
      ],
      "metadata": {
        "id": "EJQd5hlLGkPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_targets = set(flatten(train_targets))\n",
        "all_train_targets"
      ],
      "metadata": {
        "id": "VOm4ESMtGlRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_test_targets = set(flatten(test_targets))\n",
        "all_test_targets"
      ],
      "metadata": {
        "id": "S-hEqevOGmKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_train_targets == all_test_targets"
      ],
      "metadata": {
        "id": "GA3ErNqDGnMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert targets to sequences\n",
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(train_targets)\n",
        "train_targets_int = tag_tokenizer.texts_to_sequences(train_targets)\n",
        "test_targets_int = tag_tokenizer.texts_to_sequences(test_targets)\n",
        "\n",
        "# save for later\n",
        "train_targets_int_unpadded = train_targets_int\n",
        "test_targets_int_unpadded = test_targets_int"
      ],
      "metadata": {
        "id": "fRJBEQ6kGo6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# before padding, find max document length\n",
        "# because we don't want to truncate any inputs\n",
        "# which would also truncate targets\n",
        "maxlen_train = max(len(sent) for sent in train_inputs)\n",
        "maxlen_test = max(len(sent) for sent in test_inputs)\n",
        "T = max((maxlen_train, maxlen_test))"
      ],
      "metadata": {
        "id": "CMYgVB43GqaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad sequences so that we get a N x T matrix\n",
        "train_inputs_int = pad_sequences(train_inputs_int, maxlen=T)\n",
        "print('Shape of data train tensor:', train_inputs_int.shape)"
      ],
      "metadata": {
        "id": "l4CVHs9wGsag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs_int = pad_sequences(test_inputs_int, maxlen=T)\n",
        "print('Shape of data test tensor:', test_inputs_int.shape)"
      ],
      "metadata": {
        "id": "BAMg_Oz-GtaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_targets_int = pad_sequences(train_targets_int, maxlen=T)\n",
        "print('Shape of train targets tensor:', train_targets_int.shape)"
      ],
      "metadata": {
        "id": "a5nCbSF9GubC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_targets_int = pad_sequences(test_targets_int, maxlen=T)\n",
        "print('Shape of test targets tensor:', test_targets_int.shape)"
      ],
      "metadata": {
        "id": "znE1HLcBGvn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of classes\n",
        "K = len(tag_tokenizer.word_index) + 1\n",
        "K"
      ],
      "metadata": {
        "id": "eWmkkrBrGwd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "\n",
        "# We get to choose embedding dimensionality\n",
        "D = 32\n",
        "\n",
        "# Note: we actually want to the size of the embedding to (V + 1) x D,\n",
        "# because the first index starts from 1 and not 0.\n",
        "# Thus, if the final index of the embedding matrix is V,\n",
        "# then it actually must have size V + 1.\n",
        "\n",
        "i = Input(shape=(T,))\n",
        "# mask_zero=True way slower on GPU than CPU!\n",
        "x = Embedding(V + 1, D, mask_zero=True)(i)\n",
        "x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
        "# x = SimpleRNN(32, return_sequences=True)(x)\n",
        "x = Dense(K)(x)\n",
        "\n",
        "model = Model(i, x)"
      ],
      "metadata": {
        "id": "2FTlYIoZGx1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and fit\n",
        "model.compile(\n",
        "  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 60s per epoch on CPU\n",
        "\n",
        "print('Training model...')\n",
        "r = model.fit(\n",
        "  train_inputs_int,\n",
        "  train_targets_int,\n",
        "  epochs=5,\n",
        "  validation_data=(test_inputs_int, test_targets_int)\n",
        ")"
      ],
      "metadata": {
        "id": "M_VFN1X0GzoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss per iteration\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "OoorBJJ9G06n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy per iteration\n",
        "plt.plot(r.history['accuracy'], label='train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "uVvw_2mQG26p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True model accuracy - above includes padding\n",
        "\n",
        "# first get length of each sequence\n",
        "train_lengths = []\n",
        "for sentence in train_inputs:\n",
        "  train_lengths.append(len(sentence))\n",
        "\n",
        "test_lengths = []\n",
        "for sentence in test_inputs:\n",
        "  test_lengths.append(len(sentence))"
      ],
      "metadata": {
        "id": "A-3kfWvBG4Rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_probs = model.predict(train_inputs_int) # N x T x K\n",
        "train_predictions = []\n",
        "for probs, length in zip(train_probs, train_lengths):\n",
        "  # probs is T x K\n",
        "  probs_ = probs[-length:]\n",
        "  preds = np.argmax(probs_, axis=1)\n",
        "  train_predictions.append(preds)\n",
        "\n",
        "# flatten\n",
        "flat_train_predictions = flatten(train_predictions)\n",
        "flat_train_targets = flatten(train_targets_int_unpadded)"
      ],
      "metadata": {
        "id": "AuLbGh-dG6Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_probs = model.predict(test_inputs_int) # N x T x K\n",
        "test_predictions = []\n",
        "for probs, length in zip(test_probs, test_lengths):\n",
        "  # probs is T x K\n",
        "  probs_ = probs[-length:]\n",
        "  preds = np.argmax(probs_, axis=1)\n",
        "  test_predictions.append(preds)\n",
        "\n",
        "# flatten\n",
        "flat_test_predictions = flatten(test_predictions)\n",
        "flat_test_targets = flatten(test_targets_int_unpadded)"
      ],
      "metadata": {
        "id": "IVl8Ggt3G-8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "print(\"Train acc:\", accuracy_score(flat_train_targets, flat_train_predictions))\n",
        "print(\"Test acc:\", accuracy_score(flat_test_targets, flat_test_predictions))\n",
        "\n",
        "print(\"Train f1:\", f1_score(flat_train_targets, flat_train_predictions, average='macro'))\n",
        "print(\"Test f1:\", f1_score(flat_test_targets, flat_test_predictions, average='macro'))"
      ],
      "metadata": {
        "id": "BisI-EuGHACu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Baseline model: map word to tag\n",
        "from collections import Counter\n",
        "\n",
        "# https://stackoverflow.com/questions/1518522/find-the-most-common-element-in-a-list\n",
        "def most_common(lst):\n",
        "    data = Counter(lst)\n",
        "    return data.most_common(1)[0][0]"
      ],
      "metadata": {
        "id": "vrfy6vgIHBMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token2tags = {k: [] for k, v in word2idx.items()}\n",
        "\n",
        "# remove UNK token\n",
        "del token2tags['UNK']\n",
        "\n",
        "for tokens, tags in zip(train_inputs, train_targets):\n",
        "  for token, tag in zip(tokens, tags):\n",
        "    if should_lowercase:\n",
        "      token = token.lower()\n",
        "    if token in token2tags:\n",
        "      token2tags[token].append(tag)\n",
        "\n",
        "for k, v in token2tags.items():\n",
        "  if len(v) == 0:\n",
        "    print(k)\n",
        "\n",
        "token2tag = {k: most_common(v) for k, v in token2tags.items()}"
      ],
      "metadata": {
        "id": "S92QiznrHDby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute accuracy\n",
        "\n",
        "train_predictions = []\n",
        "for sentence in train_inputs:\n",
        "  predictions = []\n",
        "  for token in sentence:\n",
        "    if should_lowercase:\n",
        "      token = token.lower()\n",
        "    predicted_tag = token2tag[token]\n",
        "    predictions.append(predicted_tag)\n",
        "  train_predictions.append(predictions)\n",
        "flat_train_predictions = flatten(train_predictions)\n",
        "flat_train_targets = flatten(train_targets)"
      ],
      "metadata": {
        "id": "5Z-Q6PcbHD0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = []\n",
        "for sentence in test_inputs:\n",
        "  predictions = []\n",
        "  for token in sentence:\n",
        "    predicted_tag = token2tag.get(token, 'INCORRECT')\n",
        "    predictions.append(predicted_tag)\n",
        "  test_predictions.append(predictions)\n",
        "flat_test_predictions = flatten(test_predictions)\n",
        "flat_test_targets = flatten(test_targets)"
      ],
      "metadata": {
        "id": "RHUsao3XHE3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train acc:\", accuracy_score(flat_train_targets, flat_train_predictions))\n",
        "print(\"Test acc:\", accuracy_score(flat_test_targets, flat_test_predictions))\n",
        "\n",
        "print(\"Train f1:\", f1_score(flat_train_targets, flat_train_predictions, average='macro'))\n",
        "print(\"Test f1:\", f1_score(flat_test_targets, flat_test_predictions, average='macro'))"
      ],
      "metadata": {
        "id": "iPXvrSwJHGBy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}