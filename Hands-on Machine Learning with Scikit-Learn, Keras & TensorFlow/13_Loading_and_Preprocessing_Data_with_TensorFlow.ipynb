{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Data API"
      ],
      "metadata": {
        "id": "vis959Sek2-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.range(10)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
        "dataset"
      ],
      "metadata": {
        "id": "cu-U4KtKlDcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in dataset:\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "NCSPj2qrlIh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chaining Transformations"
      ],
      "metadata": {
        "id": "4PuwMl5qlKyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.repeat(3).batch(7)\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "n5Fd9qyGlON2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(lambda x: x * 2)"
      ],
      "metadata": {
        "id": "-HhGO3QDlThL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.apply(tf.data.experimental.unbatch())"
      ],
      "metadata": {
        "id": "sr2jDMNslb-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.filter(lambda x: x < 10)"
      ],
      "metadata": {
        "id": "0-P1EvOslgKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in dataset.take(3):\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "-9aW_-EMljs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shuffling the Data"
      ],
      "metadata": {
        "id": "mjf5H1Lwlm-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.range(10).repeat(3)\n",
        "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "dRZziGBUlpMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interleaving Lines From Multiple Files"
      ],
      "metadata": {
        "id": "k2Vn-qvrlx5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_filepaths = [\"..\", \"..\"]\n",
        "\n",
        "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
      ],
      "metadata": {
        "id": "XqlxTGqKl6ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_readers = 5\n",
        "dataset = filepath_dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length = n_readers\n",
        ")\n",
        "\n",
        "for line in dataset.take(5):\n",
        "  print(line.numpy())"
      ],
      "metadata": {
        "id": "3nR6XMt5mHsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Data"
      ],
      "metadata": {
        "id": "zI5rrLc4mYI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_mean, X_std = [...]\n",
        "n_inputs = 8\n",
        "\n",
        "def preprocess(line):\n",
        "  defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
        "  fields = tf.io.decode_csv(line, record_defaults=defs)\n",
        "  x = tf.stack(fields[:-1])\n",
        "  y = tf.stack(fields[-1:])\n",
        "  return (x - X_mean) / X_std, y"
      ],
      "metadata": {
        "id": "nq3ebJxTnOKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(b\"4.2083, 44.0, 5.3232, 0.9171, 846.0, 2.3370, 37.47, -122.2, 2.782\")"
      ],
      "metadata": {
        "id": "zXCDtYTFnl1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting Everything Together"
      ],
      "metadata": {
        "id": "MSnlPr_mnwx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_reader_dataset(filepaths, repeat=None, n_readers=5,\n",
        "                       n_read_thread=None, shuffle_buffer_size=10000,\n",
        "                       n_parse_threads=5, batch_size=32):\n",
        "  dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
        "  dataset = dataset.interleave(\n",
        "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
        "    cycle_length = n_readers, num_parallel_calls=n_read_threads)\n",
        "  dataset = dataset.shuffle(shuffle_buffer_size)\n",
        "  dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "GrRAevgRn0fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prefetching"
      ],
      "metadata": {
        "id": "4aTbgeKaokeR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the Dataset With tf.keras"
      ],
      "metadata": {
        "id": "xnQUXhJdolrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = csv_reader_dataset(train_filepaths, repeat=None)\n",
        "valid_set = csv_reader_dataset(valid_filepaths)\n",
        "test_set = csv_reader_dataset(test_filepaths)"
      ],
      "metadata": {
        "id": "uLM-_bA8orfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([...])\n",
        "model.compile([...])\n",
        "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n",
        "          validation_data = valid_set,\n",
        "          validation_steps = len(X_valid) // batch_size)"
      ],
      "metadata": {
        "id": "2g9zw_vQo3E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_set, steps=len(X_test) // batch_size)\n",
        "model.predic(new_set, steps=len(X_new) // batch_size)"
      ],
      "metadata": {
        "id": "PyaNPdFppPcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loss_fn, n_epochs, [...]):\n",
        "  train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])\n",
        "  for X_batch, y_batch in train_set:\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = model(X_batch)\n",
        "      main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
        "      loss = tf.add_n([main_loss] + model.losses)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "_NUIybjppjAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The TFRecord Format"
      ],
      "metadata": {
        "id": "xEf9HMmbqK2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
        "  f.write(b\"This is the first record\")\n",
        "  f.write(b\"And this is the second record\")"
      ],
      "metadata": {
        "id": "ezDbtnBAqPN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepaths = [\"my_data.tfrecord\"]\n",
        "dataset = tf.data.TFRecordDataset(filepaths)\n",
        "for item in dataset:\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "bVIp1EiiqbYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compressed TFRecord Files"
      ],
      "metadata": {
        "id": "yYj30SbIqipT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "options = tf.io.TFRecordOptions(compression_type=\"GZIP\")\n",
        "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
        "  [...]"
      ],
      "metadata": {
        "id": "xnX_CFyOqnKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
        "                                  compression_type=\"GZIP\")"
      ],
      "metadata": {
        "id": "z8Pl-0MSq1lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Brief Introduction to Protocol Buffers"
      ],
      "metadata": {
        "id": "tVLJjbBaq-Lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "syntax = \"proto3\";\n",
        "message Person {\n",
        "    string name = 1;\n",
        "    int32 id = 2;\n",
        "    repeated string email = 3;\n",
        "}\n",
        "\n",
        "from person_pb2 import Person\n",
        "person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])\n",
        "print(person)"
      ],
      "metadata": {
        "id": "HiUyxpI8rKa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person.name"
      ],
      "metadata": {
        "id": "asIFO98NreRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person.name = \"Alice\""
      ],
      "metadata": {
        "id": "aLfqpc5jrffd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person.email[0]"
      ],
      "metadata": {
        "id": "aeWAT1HQrhbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person.email.append(\"c@d.com\")\n",
        "s = person.SerializeToString()\n",
        "s"
      ],
      "metadata": {
        "id": "wEBmtfzqri4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person2 = Person()\n",
        "person2.ParseFromString(s)"
      ],
      "metadata": {
        "id": "ky66aaJ8rqlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "person == person2"
      ],
      "metadata": {
        "id": "zorMg_n2rvuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Features API"
      ],
      "metadata": {
        "id": "5F226iEur0Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_median_age = tf.feature_column.numeric_column(\"housing_median_age\")"
      ],
      "metadata": {
        "id": "LUypTnEZsADJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_mean, age_std = X_mean[1], X_std[1]\n",
        "housing_median_age = tf.feature_column.numeric_column(\n",
        "    \"housing_median_age\", normalizer_fn=lambda x: (x - age_mean) / age_std)"
      ],
      "metadata": {
        "id": "W8ovdrKqsGms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
        "bucketized_income = tf.feature_column.bucketized_column(\n",
        "    median_income, boundaries=[1.5, 3., 4.5, 6.]\n",
        ")"
      ],
      "metadata": {
        "id": "7o0tk0Q-sWxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical Features"
      ],
      "metadata": {
        "id": "wM47hPxSs5_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ocean_prox_vocab = [\"<1H OCEAN\", \"INLAND\", \"ISLAND\", \"NEAR BAY\", \"NEAR OCEAN\"]\n",
        "ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "    \"ocean_proximity\", ocean_prox_vocab\n",
        ")"
      ],
      "metadata": {
        "id": "PE6Sm_XIs7j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_hash = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "    \"city\", hash_bucket_size=1000\n",
        ")"
      ],
      "metadata": {
        "id": "JELxw5nAtQf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crossed Categorical Features"
      ],
      "metadata": {
        "id": "CFgm_Dp0tX_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucketized_age = tf.feature_column.bucketized_column(\n",
        "    housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.])\n",
        "age_and_ocean_proximity = tf.feature_column.crossed_column(\n",
        "    [bucketized_age, ocean_proximity], hash_bucket_size=100)"
      ],
      "metadata": {
        "id": "ubGaiKYmtdEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latitude = tf.feature_column.numeric_column(\"latitude\")\n",
        "longitude = tf.feature_column.numeric_column(\"longitude\")\n",
        "bucketized_latitude = tf.feature_column.bucketized_column(\n",
        "    latitude, boundaries=list(np.linspace(32., 42., 20 - 1))\n",
        ")\n",
        "bucketized_longitude = tf.feature_column.bucketized_column(\n",
        "    longitude, boundaries=list(np.linspace(-125., -114., 20 - 1))\n",
        ")\n",
        "location = tf.feature_column.crossed_column(\n",
        "    [bucketized_latitude, bucketized_longitude], hash_bucket_size=1000\n",
        ")"
      ],
      "metadata": {
        "id": "3wevyBcjt0o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Categorical Features Using One-Hot Vectors"
      ],
      "metadata": {
        "id": "zyi0bdjHua-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity)"
      ],
      "metadata": {
        "id": "JLi3LOMaueur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Categorical Features Using Embeddings"
      ],
      "metadata": {
        "id": "Xp25SU8Vulni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity,\n",
        "                                                           dimension=2)"
      ],
      "metadata": {
        "id": "PVKkCPYHuqFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Feature Columns for Parsing"
      ],
      "metadata": {
        "id": "qI-TZrANuyyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = [bucketized_age, ......, median_house_value] # all features + target\n",
        "feature_descriptions = tf.feature_column.make_parse_example_spec(columns)"
      ],
      "metadata": {
        "id": "YVDibeNju1fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_examples(serialized_examples):\n",
        "  examples = tf.io.parse_example(serialized_examples, feature_descriptions)\n",
        "  targets = examples.pop(\"median_house_value\")\n",
        "  return examples, targets"
      ],
      "metadata": {
        "id": "GH9Sx-BjvBT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "dataset = tf.data.TFRecordDataset([\"my_data_with_features.tfrecords\"])\n",
        "dataset = dataset.repeat().shuffle(10000).batch(batch_size).map(parse_examples)"
      ],
      "metadata": {
        "id": "7VA4wwhyvM3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Feature Columns in Your Models"
      ],
      "metadata": {
        "id": "pq8hxopMvYEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_without_target = columns[:-1]\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.DenseFeatures(feature_columns=columns_without_target),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mse\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "steps_per_epoch = len(X_train) // batch_size\n",
        "history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=5)"
      ],
      "metadata": {
        "id": "fL3OdqG4va3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "some_columns = [ocean_proximity_embed, bucketized_income]\n",
        "dense_features = keras.layers.DenseFeatures(some_columns)\n",
        "dense_features({\n",
        "    \"ocean_proximity\": [[\"NEAR OCEAN\"], [\"INLAND\"], [\"INLAND\"]],\n",
        "    \"median_income\": [[3.], [7.2], [1.]]\n",
        "})"
      ],
      "metadata": {
        "id": "ocylqxbwvz0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF Transform"
      ],
      "metadata": {
        "id": "xATXJGUAwIrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_transform as tft\n",
        "\n",
        "def preprocess(inputs): # inputs is a batch of input features\n",
        "  median_age = inputs[\"housing_median_age\"]\n",
        "  ocean_proximity = inputs[\"ocean_proximity\"]\n",
        "  standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age))\n",
        "  ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
        "  return {\n",
        "      \"standardized_median_age\": standardized_age,\n",
        "      \"ocean_proximity_id\": ocean_proximity_id\n",
        "  }"
      ],
      "metadata": {
        "id": "NPfUcL2MwK9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The TensorFlow Datasets (TFDS) Project"
      ],
      "metadata": {
        "id": "bG3BdmdKw3Et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "dataset = tfds.load(name=\"mnist\")\n",
        "mnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]"
      ],
      "metadata": {
        "id": "xU72yJLhw8Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\n",
        "for item in mnist_train:\n",
        "  images = item[\"image\"]\n",
        "  labels = item[\"label\"]\n",
        "  ..."
      ],
      "metadata": {
        "id": "4NIONaIKxFuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train = mnist_train.repeat(5).batch(32)\n",
        "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\n",
        "mnist_train = mnist_train.prefetch(1)"
      ],
      "metadata": {
        "id": "CgQRvWC5xThr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\n",
        "mnist_train = dataset[\"train\"].repeat().prefetch(1)\n",
        "model = keras.models.Sequential([...])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\n",
        "model.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=5)"
      ],
      "metadata": {
        "id": "JYLFsFokxjEd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}