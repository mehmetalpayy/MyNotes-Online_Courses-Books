{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 7.1 Going beyond the Sequential model: the Keras functional API"
      ],
      "metadata": {
        "id": "8U8bMY5tljGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1.1 Introduction to the functional API"
      ],
      "metadata": {
        "id": "MVMgWvi1l56e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Input, layers\n",
        "input_tensor = Input(shape=(32,)) # A tensor\n",
        "dense = layers.Dense(32, activation=\"relu\") # A layer is a function\n",
        "output_tensor = dense(input_tensor) # A layer may be called on a tensor, and it returns a tensor"
      ],
      "metadata": {
        "id": "RWGWvXWjl9so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sied by side a simple Sequential model and its equivalent in the functional API:"
      ],
      "metadata": {
        "id": "3F4-mPU4nxt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "\n",
        "seq_model = Sequential()\n",
        "seq_model.add(layers.Dense(32, activation=\"relu\", input_shape=(64,)))\n",
        "seq_model.add(layers.Dense(32, activation=\"relu\"))\n",
        "seq_model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "input_tensor = Input(shape=(64,))\n",
        "x = layers.Dense(32, activation=\"relu\")(input_tensor)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "output_tensor = layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(input_tensor, output_tensor)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "mIF_i9OUmyvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason it works is that output_tensor was obtained by repeatedly transforming input_tensor. If you tried to build a model from inputs and outputs that weren't related, you'd get a RuntimeError"
      ],
      "metadata": {
        "id": "j2rpvQhdqOsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\") # Compiles the model\n",
        "\n",
        "import numpy as np\n",
        "x_train = np.random.random((1000, 64))\n",
        "y_train = np.random.random((1000, 10))\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128) # Trains the model for 10 epochs\n",
        "\n",
        "score = model.evaluate(x_train, y_train) # Evaluates the model"
      ],
      "metadata": {
        "id": "v5Ol72YOn3WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1.2 Multi-input models"
      ],
      "metadata": {
        "id": "Ps6RHGp0rGrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functional API implementation of a two-input question-answering model\n",
        "\n",
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "\n",
        "text_vocabulary_size = 10000\n",
        "question_vocabulary_size = 10000\n",
        "answer_vocabulary_size = 500\n",
        "\n",
        "text_input = Input(shape=(None,), dtype=\"int32\", name=\"text\") # The text input is a variable-length sequence of integers.\n",
        "\n",
        "embedded_text = layers.Embedding(64, text_vocabulary_size)(text_input) # Embeds the inputs into a sequence of vectors of size 64\n",
        "\n",
        "encoded_text = layers.LSTM(32)(embedded_text) # Encodes the vectors in a single vector via an LSTM\n",
        "\n",
        "question_input = Input(shape=(None,),\n",
        "                       dtype=\"int32\",\n",
        "                       name=\"question\")\n",
        "\n",
        "embedded_question = layers.Embedding(32, question_vocabulary_size)(question_input)\n",
        "\n",
        "encoded_question = layers.LSTM(16)(embedded_question)\n",
        "\n",
        "concatenated = layers.concatenate([encoded_text, encoded_question], axis=-1) # Concatenates the encoded question and encoded text\n",
        "\n",
        "answer = layers.Dense(answer_vocabulary_size, activation=\"softmax\")(concatenated) # Adds a softmax classifier on top\n",
        "\n",
        "model = Model([text_input, question_input], answer) # At model instantiation, you specify the two inputs and the output\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"acc\"])"
      ],
      "metadata": {
        "id": "rAqp_0-lrda4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feeding data to a multi-input model\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "num_samples = 1000\n",
        "max_length = 100\n",
        "\n",
        "text = np.random.randint(1, text_vocabulary_size, size=(num_samples, max_length)) # Generates dummy Numpy data\n",
        "\n",
        "question = np.random.randint(1, question_vocabulary_size, size=(num_samples, max_length))\n",
        "\n",
        "answer = np.random.randint(0, 1, size=(num_samples, answer_vocabulary_size)) # Answers are one-hot encoded, not integers\n",
        "\n",
        "model.fit([text, question], answers, epochs=10, batch_size=128) # Fitting using a list of inputs\n",
        "\n",
        "model.fit({\"text\": text, \"question\": question}, answers, epochs=10, batch_size=128) # Fitting using a dictionary of inputs (only if inputs are named)"
      ],
      "metadata": {
        "id": "A443v58Uwhtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1.3 Multi-output models"
      ],
      "metadata": {
        "id": "rHCxhQddxlk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functional API implementation of a three-output model\n",
        "\n",
        "vocabulary_size = 50000\n",
        "num_income_groups = 10\n",
        "\n",
        "posts_input = Input(shape=(None,), dtype=\"int32\", name=\"posts\")\n",
        "embedded_posts = layers.Embedding(256, vocabulary_size)(posts_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_posts)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(256, 5, activation=\"relu\")(x)\n",
        "x = layers.Conv1D(256, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(256, 5, activation=\"relu\")(x)\n",
        "x = layers.Conv1D(256, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "\n",
        "age_prediction = layers.Dense(1, name=\"age\")(x)\n",
        "income_prediction = layers.Dense(num_income_groups,\n",
        "                                 activation=\"softmax\",\n",
        "                                 name=\"income\")(x)\n",
        "\n",
        "gender_prediction = layers.Dense(1, activation=\"sigmoid\", name=\"gender\")(x)\n",
        "\n",
        "model = Model(posts_input, [age_prediction, income_prediction, gender_prediction])"
      ],
      "metadata": {
        "id": "gctC5zecyAoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilation options of a multi-output model: multiple losses\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=[\"mse\", \"categorical_crossentropy\", \"binary_crossentropy\"])\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss={\"age\": \"mse\",\n",
        "                    \"income\": \"categorical_crossentropy\",\n",
        "                    \"gender\": \"binary_crossentropy\"})"
      ],
      "metadata": {
        "id": "VPQ1NKKf0Qt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilation options of a multi-output model: loss weighting\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=[\"mae\", \"categorical_crossentropy\", \"binary_crossentropy\"],\n",
        "              loss_weights=[0.25, 1., 10.])\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss={\"age\": \"mse\",\n",
        "                     \"income\": \"categorical_crossentropy\",\n",
        "                     \"gender\": \"binary_crossentropy\"},\n",
        "              loss_weights={\"age\": 0.25,\n",
        "                            \"income\": 1.,\n",
        "                            \"gender\": 10.})"
      ],
      "metadata": {
        "id": "GxPhUXWk2cl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feeding data to a multi-output model\n",
        "\n",
        "model.fit(posts, [age_targets, income_targets, gender_targets], epochs=10, batch_size=64)\n",
        "\n",
        "model.fit(posts, {\"age\": age_targets,\n",
        "                  \"income\": income_targets,\n",
        "                  \"gender\": gender_targets}, epochs=10, batch_size=64)"
      ],
      "metadata": {
        "id": "w_4OalDn3KpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1.4 Directed acyclic graphs of layers"
      ],
      "metadata": {
        "id": "_0-wszUz3hAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "branch_a = layers.Conv2D(128, 1, activation=\"relu\", strides=2)(x)\n",
        "\n",
        "branch_b = layers.Conv2D(128, 1, activation=\"relu\")(x)\n",
        "branch_b = layers.Conv2D(128, 3, activation=\"relu\", strides=2)(branch_b)\n",
        "\n",
        "branch_c = layers.AveragePooling2D(3, strides=2)(x)\n",
        "branch_c = layers.Conv2D(128, 3, activation=\"relu\")(branch_c)\n",
        "\n",
        "branch_d = layers.Conv2D(128, 1, activation=\"relu\")(x)\n",
        "branch_d = layers.Conv2D(128, 3, activation=\"relu\")(branch_d)\n",
        "branch_d = layers.Conv2D(128, 3, activation=\"relu\", strides=2)(branch_d)\n",
        "\n",
        "output = layers.concatenate([branch_a, branch_b, branch_c, branch_d], axis=-1) # Concatenates the branch outputs to obtain the module output"
      ],
      "metadata": {
        "id": "QQWFzn633kAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to implement a residual connection in Keras"
      ],
      "metadata": {
        "id": "6mEDlN1N7O2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = ....\n",
        "y = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "y = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(y)\n",
        "y = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(y)\n",
        "\n",
        "y = layers.add([y, x]) # Adds the original x back to the output features"
      ],
      "metadata": {
        "id": "7WIVCAXH7Gtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1.5 Layer weight sharing"
      ],
      "metadata": {
        "id": "bPDIwY3n7kQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = layers.LSTM(32)\n",
        "\n",
        "left_input = Input(shape=(None, 128))\n",
        "left_output = lstm(left_input)\n",
        "\n",
        "right_input = Input(shape=(None, 128))\n",
        "right_output = lstm(right_input)\n",
        "\n",
        "merged = layers.concatenate([left_output, right_output], axis=-1)\n",
        "predictions = layers.Dense(1, activation=\"sigmoid\")(merged)\n",
        "\n",
        "model = Model([left_input, right_input], predictions)\n",
        "model.fit([left_data, right_data], targets)"
      ],
      "metadata": {
        "id": "NVs-6Kwf7s3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1.6 Models as layers"
      ],
      "metadata": {
        "id": "hg6tfppH8RWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras import applications\n",
        "from keras import Input\n",
        "\n",
        "xception_base = applications.Xception(weights=None,\n",
        "                                      include_top=False)\n",
        "\n",
        "left_input = Input(shape=(250, 250, 3))\n",
        "right_input = Input(shape=(250, 250, 3))\n",
        "\n",
        "left_features = xception_base(left_input)\n",
        "right_input = xception_base(right_input)\n",
        "\n",
        "merged_features = layers.concatenate([left_features, right_input], axis=-1)"
      ],
      "metadata": {
        "id": "_asqQ1Fp8UY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.2 Inspecting and monitoring deep-learning models using Keras callbacks and Tensorboard"
      ],
      "metadata": {
        "id": "kg1Ac68Y81xQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2.1 Using callbacks to act on a model during training"
      ],
      "metadata": {
        "id": "aAktsvyg856_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The MODELCHECKPOINT and EARLYSTOPPING callbacks\n",
        "\n",
        "import keras\n",
        "\n",
        "callbacks_list = [\n",
        "    keras.callbacks.EarlyStopping( # Interrupts training when improvement stops\n",
        "        monitor=\"acc\", # Monitors the model's validation accuracy\n",
        "        patience=1, # Interrupts training when accuracy has stopped improving for more than one epoch\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint( # Saves the current weights after every epoch\n",
        "        filepath = \"my_model.h5\", # Path to the destination model file\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "model.fit(x, y,\n",
        "          epochs=10,\n",
        "          batch_size=32,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "dz3eU7sN8-On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The REDUCELRONPLATEU callback\n",
        "\n",
        "callbacks_list = [\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"loss\", # Monitors the model's validation loss\n",
        "        factor=0.1, # Divides the learning rate by 10 when triggered\n",
        "        patience=10, # the callback is triggered after the validation loss has stopped improving for 10 epochs.\n",
        "    )\n",
        "]\n",
        "\n",
        "model.fit(x, y,\n",
        "          epochs=10,\n",
        "          batch_size=32,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "P4NnXXsh93mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing own callback\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "class ActivationLonger(keras.callbacks.Callback):\n",
        "\n",
        "  def set_model(self, model):\n",
        "    self.model = model\n",
        "    layer_outputs = [layer.output for layer in model.layers]\n",
        "    self.activations_model = keras.models.Model(model.input, layer_outputs)\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if self.validation_data is None:\n",
        "      raise RuntimeError(\"Requires validation_data.\")\n",
        "\n",
        "    validation_sample = self.validation_data[0][0:1]\n",
        "    activations = self.activations_model.predict(validation_sample)\n",
        "    f = open(\"activations_at_epoch_\" + str(epoch) + \".npz\", \"w\")\n",
        "    np.savez(f, activations)\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "nh_Oi14r-aaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2.2 Introduction to TensorBoard: the TensorFlow visualization framework"
      ],
      "metadata": {
        "id": "Wu82DBYi_LUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text-classification model to use with TensorBoard\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 2000\n",
        "max_len = 500\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=maxlen, name=\"embed\"))\n",
        "model.add(layers.Conv1D(32, 7, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation=\"relu\"))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"acc\"])\n"
      ],
      "metadata": {
        "id": "HnI3MUVq_RlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model with a TensorBoard callback\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.TensorBoard(\n",
        "        log_dir = \"my_log_dir\",\n",
        "        histogram_freq = 1,\n",
        "        emedding_freq = 1,\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "id": "HrTIeucAAJrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.3 Getting the most out of your models"
      ],
      "metadata": {
        "id": "nKV3tseOAc66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3.1 Advanced architecture patterns"
      ],
      "metadata": {
        "id": "Dfot6nFTAiU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Normalization\n",
        "\n",
        "normalized_data = (data - np.mean(data, axis=..)) / np.std(data, axis=...)"
      ],
      "metadata": {
        "id": "UwQgU_AhAnbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The BatchNormalization layer is typically used after a convolutional or densely connected layer\n",
        "\n",
        "conv_model.add(layers.Conv2D(32, 3, activations=\"relu\"))\n",
        "conv_model.add(layers.BatchNormalization())\n",
        "\n",
        "dense_model.add(layers.Dense(32, activation=\"relu\"))\n",
        "dense_model.add((layers.BatchNormalization()))"
      ],
      "metadata": {
        "id": "teRW9cJMAwWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Depthwise Separable Convolution\n",
        "\n",
        "height = 64\n",
        "width = 64\n",
        "channels = 3\n",
        "num_classes = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.SeparableConv2D(32, 3,\n",
        "                                 activation=\"relu\",\n",
        "                                 input_shape=(height, width, channels,)))\n",
        "model.add(layers.SeparableConv2D(64, 3, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(2))\n",
        "\n",
        "model.add(layers.SeparableConv2D(64, 3, activation=\"relu\"))\n",
        "model.add(layers.SeparableConv2D(128, 3, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(2))\n",
        "\n",
        "model.add(layers.SeparableConv2D(64, 3, activation=\"relu\"))\n",
        "model.add(layers.SeparableConv2D(128, 3, activation=\"relu\"))\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "\n",
        "model.add(layers.Dense(32, activation=\"relu\"))\n",
        "model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
      ],
      "metadata": {
        "id": "_vTC4y6qBDYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3.3 Model ensembling"
      ],
      "metadata": {
        "id": "lFeEm3Y-BwNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use four different models to compute initial predictions\n",
        "\n",
        "preds_a = model_a.predict(x_val)\n",
        "preds_b = model_b.predict(x_val)\n",
        "preds_c = model_c.predict(x_val)\n",
        "preds_d = model_d.predict(x_val)\n",
        "\n",
        "final_preds = 0.25 * (preds_a + preds_b + preds_c + preds_d)"
      ],
      "metadata": {
        "id": "CZqAr0Q-ByAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_a = model_a.predict(x_val)\n",
        "preds_b = model_b.predict(x_val)\n",
        "preds_c = model_c.predict(x_val)\n",
        "preds_d = model_d.predict(x_val)\n",
        "\n",
        "final_preds = 0.5 * preds_a + 0.25 * preds_b + 0.1 * preds_c + 0.15 * preds_d"
      ],
      "metadata": {
        "id": "nGwD0KO1CA_m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}