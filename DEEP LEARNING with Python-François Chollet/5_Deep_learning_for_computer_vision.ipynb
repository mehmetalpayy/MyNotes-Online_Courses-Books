{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5o0-hvbMInI4"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from keras.applications import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import os, shutil\n",
        "import numpy as np\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.1 Introduction to convnets"
      ],
      "metadata": {
        "id": "XWGfvQUQ2AEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Convnet, it is a stack of Conv2D and MaxPooling2D layers."
      ],
      "metadata": {
        "id": "uQr-m-OBzy-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41UpJHgEzONX",
        "outputId": "5f42d96f-1b2d-42c2-ac3a-bfb7e4f8f51c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 55744 (217.75 KB)\n",
            "Trainable params: 55744 (217.75 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a classifier on top of the convnet\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation=\"relu\"))\n",
        "model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eng2I3b5z8VN",
        "outputId": "ef2dbd70-5846-4cb1-dcb1-2417e6f4f619"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 576)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                36928     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 93322 (364.54 KB)\n",
            "Trainable params: 93322 (364.54 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the convnet on MNIST images\n",
        "# from keras.datasets import mnist\n",
        "# from keras.utils import to_categorical\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x94COfFQ0YzZ",
        "outputId": "537aec2a-f5c9-49ff-eb21-29b2279936c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 67s 71ms/step - loss: 0.1796 - accuracy: 0.9436\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 61s 65ms/step - loss: 0.0491 - accuracy: 0.9850\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 55s 58ms/step - loss: 0.0328 - accuracy: 0.9895\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 55s 58ms/step - loss: 0.0246 - accuracy: 0.9921\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 64s 68ms/step - loss: 0.0193 - accuracy: 0.9944\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7dc3054a2a40>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbIxYRoW1tHx",
        "outputId": "e789fdc6-655e-43bc-bd90-35f547f54904"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0245 - accuracy: 0.9927\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9926999807357788"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1.2 The max-pooling operation\n",
        "\n",
        "2D max pooling feature map size'ını yarıya düşürüyor. Zor ilerleme kat ettiği için sadece 7x7 window'dan gelen bilgileri içerecek ve son layer'a baktığımızda 22x22x64 = 30,976 tane katsayı var ve bunu Dense Layer verdiğimizde 15.8 milyon parametre oluşur bu da çok büyük bir sayı demektir küçük bir model için."
      ],
      "metadata": {
        "id": "uDoDBNc711YP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_max_pool = models.Sequential()\n",
        "model_no_max_pool.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)))\n",
        "model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model_no_max_pool.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model_no_max_pool.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lv76C_L3D3D",
        "outputId": "f66356d0-6bb2-467a-8cc6-a1f8513eaf29"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 22, 22, 64)        36928     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 55744 (217.75 KB)\n",
            "Trainable params: 55744 (217.75 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2 Training a convnet from scratch on a small dataset"
      ],
      "metadata": {
        "id": "Kb03yZjm3bJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.2 Downloading the data"
      ],
      "metadata": {
        "id": "rqdexZRa4ez7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data link: www.kaggle.com/c/dogs-vs-cats/data"
      ],
      "metadata": {
        "id": "XmlcUDEK5VHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copying images to training, validation and test directories\n",
        "# import os, shutil\n",
        "\n",
        "original_dataset_dir = \"../Downloads/kaggle_original_data\"\n",
        "\n",
        "base_dir = \"../Downloads/cats_and_dogs_small\"\n",
        "os.mkdir(base_dir)\n",
        "\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, \"validation\")\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "os.mkdir(test_dir)\n",
        "\n",
        "train_cats_dir = os.path.join(train_dir, \"cats\")\n",
        "os.mkdir(train_cats_dir)\n",
        "\n",
        "train_dogs_dir = os.path_join(train_dir, \"dogs\")\n",
        "os.mkdir(train_dogs_dir)\n",
        "\n",
        "validation_cats_dir = os.path.join(validation_dir, \"cats\")\n",
        "os.mkdir(validation_cats_dir)\n",
        "\n",
        "validation_dogs_dir = os.path.join(validation_dir, \"dogs\")\n",
        "os.mkdir(validation_dogs_dir)\n",
        "\n",
        "test_cats_dir = os.path.join(test_dir, \"cats\")\n",
        "os.mkdir(test_cats_dir)\n",
        "test_dogs_dir = os.path.join(test_dir, \"dogs\")\n",
        "os.mkdir(test_dogs_dir)\n",
        "\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "  src = os.path.join(original_dataset_dir, fname)\n",
        "  dst = os.path.join(train_cats_dir, fname)\n",
        "  shutil.copyfile(src, dst)\n",
        "\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
        "for fname in fnames:\n",
        "  src = os.path.join(original_dataset_dir, fname)\n",
        "  dst = os.path.join(validation_cats_dir, fname)\n",
        "  shutil.copyfile(src, dst)\n",
        "\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
        "for fname in fnames:\n",
        "  src = os.path.join(original_dataset_dir, fname)\n",
        "  dst = os.path.join(test_cats_dir, fname)\n",
        "  shutil.copyfile(src, dst)\n",
        "\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "  src = os.path.join(original_dataset_dir, fname)\n",
        "  dst = os.path.join(train_dogs_dir, fname)\n",
        "  shutil.copyfile(src, dst)\n",
        "\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
        "for fname in fnames:\n",
        "  src = os.path.join(original_dataset_dir, fname)\n",
        "  dst = os.path.join(validation_dogs_dir, fname)\n",
        "  shutil.copyfile(src, dst)\n",
        "\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
        "for fname in fnames:\n",
        "  src = os.path.join(original_dataset_dir, fname)\n",
        "  dst = os.path.join(test_dogs_dir, fname)\n",
        "  shutil.copyfile(src, dst)\n",
        "\n",
        "print(\"total training cat images:\", len(os.listdir(train_cats_dir)))\n",
        "print(\"total training dog images:\", len(os.listdir(train_dogs_dir)))\n",
        "print(\"total validation cat images:\", len(os.listdir(validation_cats_dir)))\n",
        "print(\"total validation dog images:\", len(os.listdir(validation_dogs_dir)))\n",
        "print(\"total test cat images:\", len(os.listdir(test_cats_dir)))\n",
        "print(\"total test dog images:\", len(os.listdir(test_dogs_dir)))"
      ],
      "metadata": {
        "id": "2g7ri9U54jDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.3 Building the network"
      ],
      "metadata": {
        "id": "Mam1Ozem7v0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating a small convnet for dogs vs. cats classification\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(512, activation=\"relu\"))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8H5E2RL7x81",
        "outputId": "94967ba0-92a4-46ac-fa51-1bd25b965320"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 74, 74, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 36, 36, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 17, 17, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 7, 7, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 6272)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               3211776   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3453121 (13.17 MB)\n",
            "Trainable params: 3453121 (13.17 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuring the model for training\n",
        "# from keras import optimizers\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=[\"acc\"])"
      ],
      "metadata": {
        "id": "m7qoWw8A8gy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.4 Data Preprocessing"
      ],
      "metadata": {
        "id": "zFUmYCfu8-Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using ImageDataGenerator to read images from directories\n",
        "\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "test_datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "for data_batch, labels_batch in train_generator:\n",
        "  print(\"data batch shape\", data_batch.shape)\n",
        "  print(\"labels batch shape\", labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "dei9OWbC88Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Understanding Python generators\n",
        "# Iterator object, are built using \"yield\"\n",
        "\n",
        "def generators():\n",
        "  i=0\n",
        "  while True:\n",
        "    i += 1\n",
        "    yield i\n",
        "\n",
        "for item in generator():\n",
        "  print(item)\n",
        "  if item > 4:\n",
        "    break"
      ],
      "metadata": {
        "id": "cTH60_pw9mUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the model using a batch generator\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")\n",
        "\n",
        "# Saving the model\n",
        "model.save(\"cats_and_dogs_small_1.h5\")"
      ],
      "metadata": {
        "id": "F4fGEfO9-Z2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying curves of loss and accuracy during training\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history[\"acc\"]\n",
        "val_acc = history.history[\"val_acc\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc, 'b', label=\"Valdiation acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, 'b', label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UgoVrZjz-wL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.5 Using data augmentation"
      ],
      "metadata": {
        "id": "5rjZUJI1_swr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up a data augmentation configuration via ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range = 40,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = \"nearest\"\n",
        ")\n",
        "\n",
        "# rotation_range is a value in degrees(0-180), a range within which to randomly rotate pictures\n",
        "# width_shift and height_shift are ranges within which to randomly translate pictures vertically and horizontally.\n",
        "# shea_range is for randomly applying shearing transformations.\n",
        "# zoom_range is for randomly zooming inside pictures.\n",
        "# horizontal_flip is for randomly flipping half the images horizontally\n",
        "# fill_mode is the strategy used for filling in newly created pixels which can appear after a rotation or a width/height shift."
      ],
      "metadata": {
        "id": "Jk1bs4tn_yXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying some randomly augmented training images\n",
        "# from keras.preprocessing import image\n",
        "\n",
        "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
        "\n",
        "img_path = fnames[3]\n",
        "\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "x = x.reshape((1, ) + x.shape)\n",
        "\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "  plt.figure(i)\n",
        "  imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
        "  i += 1\n",
        "  if i % 4 == 0:\n",
        "    break\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gqUFS1zvCnYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a new convnet that includes dropout\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(512, activation=\"relu\"))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
        "              metrics=[\"acc\"])"
      ],
      "metadata": {
        "id": "yy3fD277Do6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the convnet using data-augmentation generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps=50\n",
        ")\n",
        "\n",
        "# Saving the model\n",
        "model.save(\"cats_nad_dogs_small_2.h5\")"
      ],
      "metadata": {
        "id": "qzwrfsGBEqN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.3 Using a pretrained convnet"
      ],
      "metadata": {
        "id": "4yBT6wtCFipL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3.1 Feature Extraction"
      ],
      "metadata": {
        "id": "R-3pNWLgF44k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating the VGG16 convolutional base\n",
        "# from keras.applications import VGG16\n",
        "\n",
        "conv_base = VGG16(weights=\"imagenet\",\n",
        "                  include_top=False,\n",
        "                  input_shape=(150, 150, 3))\n",
        "\n",
        "# weights specifies the weight checkpoint from which to initialize the model.\n",
        "# include_top refers to including(or not) the densely connected classifier on top of the network\n",
        "# input_shape is the shape of the image tensors that you'll feed to the network.\n",
        "\n",
        "conv_base.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoUyOzpvGHZW",
        "outputId": "9cd9df66-abc7-4ab8-c3de-293c4d8c2291"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 14714688 (56.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fast feature extraction without data augmentation\n",
        "\n",
        "# Extracting features using the pretrained convolutional base\n",
        "\n",
        "base_dir = \"../Downloads/cats_and_dogs_small\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "validation_dir = os.path.join(base_dir, \"validation\")\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 20\n",
        "\n",
        "def extract_features(directory, sample_count):\n",
        "  features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
        "  labels = np.zeros(shape=(sample_count))\n",
        "  generator = datagen.flow_from_director(\n",
        "      directory,\n",
        "      target_size=(150, 150),\n",
        "      batch_size=batch_size,\n",
        "      class_mode=\"binary\"\n",
        "  )\n",
        "  i = 0\n",
        "  for inputs_batch, labels_batch in generator:\n",
        "    features_batch = conv_base.predict(inputs_batch)\n",
        "    features[i * batch_size: (i + 1) * batch_size] = features_batch\n",
        "    labels[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
        "    i += 1\n",
        "    if i * batch_size >= sample_count:\n",
        "      break\n",
        "  return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(train_dir, 2000)\n",
        "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
        "test_features, test_labels = extract_features(test_dir, 1000)\n",
        "\n",
        "# The extracted features are currently of shape (samples, 4, 4, 512). We will feed them to a densely connected classifier, first flatten them to (samples, 8192)\n",
        "\n",
        "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
        "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
        "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"
      ],
      "metadata": {
        "id": "WLqKivvYG3yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining and training the densely connected classifier\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(256, activation=\"relu\", input_dim=4 * 4 * 512))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "history = model.fit(train_features, train_labels,\n",
        "                    epochs=30,\n",
        "                    batch_size=20,\n",
        "                    validation_data = (validation_features, validation_labels))"
      ],
      "metadata": {
        "id": "me08yF0iI7s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the results\n",
        "\n",
        "acc = history.history[\"acc\"]\n",
        "val_acc = history.history[\"val_acc\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc, 'b', label=\"Validation acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, 'b', label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m505BGFjKSVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction with Data Augmentation\n",
        "\n",
        "# Adding a densely connected classifier on top of the convolutional base\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation=\"relu\"))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgUJ3XcJKmIR",
        "outputId": "54af279b-0d9d-410f-bbe8-03e655082366"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 8192)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 256)               2097408   \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16812353 (64.13 MB)\n",
            "Trainable params: 16812353 (64.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"This is the number of trainable weights before freezing the conv base:\", len(model.trainable_weights))\n",
        "conv_base.trainable=False\n",
        "print(\"This is the number of trainable weights after freezing the conv base:\", len(model.trainable_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryvk9U91PY78",
        "outputId": "51474a27-0bfa-4b78-ed06-6f5c4206ea3d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the number of trainable weights before freezing the conv base: 30\n",
            "This is the number of trainable weights after freezing the conv base: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model end to end with a frozen convolutional base\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_director(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=20,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=30,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "metadata": {
        "id": "q6qtxenBPxAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3.2 Fine-tuning"
      ],
      "metadata": {
        "id": "nnx7KdFMQ0oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYROd2QBQ2_e",
        "outputId": "2b7e1cbd-6a47-4888-ae8b-efb63125a3d4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freezing all layers up to a specific one\n",
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == \"block5_conv1\":\n",
        "    set_trainable=True\n",
        "  if set_trainable:\n",
        "    layer.trainable=True\n",
        "  else:\n",
        "    layer.trainable=False\n",
        "\n",
        "# Fine-tuning the model\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=100,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=50\n",
        ")"
      ],
      "metadata": {
        "id": "DXFEfKMGQ6EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Smoothing the plots\n",
        "\n",
        "def smooth_curve(points, factor=0.8):\n",
        "  smoothed_points = []\n",
        "  for point in points:\n",
        "    if smoothed_points:\n",
        "      previous = smoothed_points[-1]\n",
        "      smoothed_points.append(previous * factor + point * (1 - factor))\n",
        "    else:\n",
        "      smoothed_points.append(point)\n",
        "  return smoothed_points\n",
        "\n",
        "plt.plot(epochs, smooth_curve(acc), \"bo\", label=\"Smoothed training acc\")\n",
        "plt.plot(epochs, smooth_curve(val_acc), \"b\", label=\"Smoothed validation acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, smooth_curve(loss), \"bo\", label=\"Smoothed training loss\")\n",
        "plt.plot(epochs, smooth_curve(val_loss), \"b\", label=\"Smoothed validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "YPp6jufqRhWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.4. Visualizing what convnets learn"
      ],
      "metadata": {
        "id": "tZ3YewDFSX-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4.1 Visualizing intermediate activations"
      ],
      "metadata": {
        "id": "dHmcbx1PSclI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.models import load_model\n",
        "\n",
        "model = load_model(\"cats_and_dogs_small_2.h5\")\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "OgEQMGEUSjJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing a single image\n",
        "img_path = \"\"\n",
        "\n",
        "img = image.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = image.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "img_tensor /= 255\n",
        "\n",
        "print(img_tensor.shape)\n",
        "\n",
        "# Displaying the test picture\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F9w3bZnlStk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating a model from an input tensor and a list of output tensors\n",
        "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
        "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
        "\n",
        "# Running the model in predict mode\n",
        "activations = activation_model.predict(img_tensor)\n",
        "\n",
        "first_layer_activation = activatios[0]\n",
        "print(first_layer_activation.shape)\n",
        "\n",
        "# Visualizing the fourth channel\n",
        "plt.matshow(first_layer_activation[0, :, :, 4], cmap=\"viridis\")\n",
        "\n",
        "# Visualizing the seventh channel\n",
        "plt.matshow(first_layer_activation[0, :, :, 7], cmap=\"viridis\")"
      ],
      "metadata": {
        "id": "KuW1OoRZTMOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing every channel in every intermediate activation\n",
        "layer_names = []\n",
        "for layer in model.layers[:8]:\n",
        "  layer_names.append(layer.name)\n",
        "\n",
        "images_per_row = 16\n",
        "\n",
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "  n_features = layer_activation.shape[-1]\n",
        "\n",
        "  size = layer_activation.shape[1]\n",
        "\n",
        "  n_cols = n_features // images_per_row\n",
        "  display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "\n",
        "  for col in range(n_cols):\n",
        "    for row in range(images_per_row):\n",
        "      channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
        "\n",
        "      channel_image -= channel_image.mean()\n",
        "      channel_image /= channel_image.std()\n",
        "      channel_image *= 64\n",
        "      channel_image += 128\n",
        "      channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
        "      display_grid[col * size: (col + 1) * size, row * size : (row + 1) * size] = channel_image\n",
        "\n",
        "scale = 1. / size\n",
        "plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                    scale * display_grid.shape[0]))\n",
        "plt.title(layer_name)\n",
        "plt.grid(False)\n",
        "plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")"
      ],
      "metadata": {
        "id": "zpIbKMmaT0vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing convnet filters"
      ],
      "metadata": {
        "id": "e6gUxSmzVGWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the loss tensor for filter visualization\n",
        "model = VGG16(weights=\"imagenet\",\n",
        "              include_top=False)\n",
        "\n",
        "layer_name = \"block3_conv1\"\n",
        "filter_index = 0\n",
        "\n",
        "layer_output = model.get_layer(layer_name).output\n",
        "loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "# Obtaining the gradient of the loss with regard to the input\n",
        "grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "# Gradient-normalization trick\n",
        "grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)"
      ],
      "metadata": {
        "id": "yi0ekxpeVKnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching numpy output values given numpy input values\n",
        "iterate = K.function([model.input], [loss, grads])\n",
        "loss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])\n",
        "\n",
        "# Loss maximization via stochastic gradient descent\n",
        "input_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128.\n",
        "\n",
        "step = 1.\n",
        "for i in range(40):\n",
        "  loss_value, grads_value = iterate([input_img_data])\n",
        "\n",
        "  input_img_data += grads_value * step\n",
        "\n",
        "# Utility function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "  x -= x.mean()\n",
        "  x /= (x.std() + 1e-5)\n",
        "  x *= 0.1\n",
        "\n",
        "  x += 0.5\n",
        "  x = np.clip(x, 0, 1)\n",
        "\n",
        "  x *= 255\n",
        "  x = np.clip(x, 0, 255).astype(\"uint8\")\n",
        "  return x"
      ],
      "metadata": {
        "id": "9nvvtDHLVwwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate filter visualizations\n",
        "\n",
        "def generate_pattern(layer_name, filter_index, size=150):\n",
        "  layer_output = model.get_layer(layer_name).output\n",
        "  loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "  grads = K.gradients(loss, model_input)[0]\n",
        "\n",
        "  grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "  iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "  input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n",
        "\n",
        "  step = 1.\n",
        "  for i in range(40):\n",
        "    loss_value, grads_value = iterate([input_img_data])\n",
        "    input_img_data += grads_value * step\n",
        "\n",
        "  img = input_img_data[0]\n",
        "  return deprocess_image(img)\n",
        "\n",
        "plt.imshow(generate_pattern(\"block3_conv1\", 0))"
      ],
      "metadata": {
        "id": "VvD6zX0jWy9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a grid of all filter response patterns in a layer\n",
        "layer_name = \"block1_conv1\"\n",
        "size=64\n",
        "margin=5\n",
        "\n",
        "results = np.zeros((8 * size + 7 * margin, 8 * size + 7 * margin, 3))\n",
        "\n",
        "for i in range(8):\n",
        "  for j in range(8):\n",
        "    filter_img = generate_pattern(layer_name, i + (j * 8), size=size)\n",
        "\n",
        "    horizontal_start = i * size + i * margin\n",
        "    horizontal_end = horizontal_start + size\n",
        "    vertical_start = j * size + j * margin\n",
        "    vertical_end = vertical_start + size\n",
        "    results[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img"
      ],
      "metadata": {
        "id": "Mcp7gjEsXppM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4.3. Visualizing heatmaps of class activation"
      ],
      "metadata": {
        "id": "F7verwuAYWIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the VGG16 network with pretrained weights\n",
        "model = VGG16(weights=\"imagenet\")\n",
        "\n",
        "# Preprocessing an input image for VGG16\n",
        "img_path = \"../Downloads/creative_commons_elephant.jpg\"\n",
        "\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "x = np.expand_dims(x, axis=0)\n",
        "\n",
        "x = preprocess_input(x)\n",
        "\n",
        "preds = model.predict(x)\n",
        "print(\"Predicted:\", decode_predictions(preds, top=3)[0])"
      ],
      "metadata": {
        "id": "SqOMjpFYYahh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the Grad-CAM algorithm\n",
        "african_elephant_output = model.output[:, 386]\n",
        "last_conv_layer = model.get_layer(\"block5_conv3\")\n",
        "\n",
        "grads = K.gradients(african_elephant_output, last_conv_layer.output)[0]\n",
        "\n",
        "pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
        "\n",
        "pooled_grads_value, conv_layer_output_value = iterate([x])\n",
        "\n",
        "for i in range(512):\n",
        "  conv_layer_output_value[:, :, i] *= pooled_grads_values[i]\n",
        "\n",
        "heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
        "\n",
        "# Heatmap post-processing\n",
        "heatmap = np.maximum(heatmap, 0)\n",
        "heatmap /= np.max(heatmap)\n",
        "plt.matshow(heatmap)\n"
      ],
      "metadata": {
        "id": "QZOhwIDXZGfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Superimposing the heatmap with the original picture\n",
        "img = cv2.imrea(img_path)\n",
        "heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "superimposed_img = heatmap * 0.4 + img\n",
        "cv2.imwrite(\"../Downloads/elephant_cam.jpg\", superimposed_img)"
      ],
      "metadata": {
        "id": "N7ex9TkKZ9a2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}