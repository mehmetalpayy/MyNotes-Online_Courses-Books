{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 6.1 Working with text data"
      ],
      "metadata": {
        "id": "68-Z3ggQ8PfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.1 One-hot encoding of words and characters"
      ],
      "metadata": {
        "id": "H4Bp-DGnB4aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word-level one-hot encoding (toy example)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      token_index[word] = len(token_index) + 1 # assigns unique index to each unique word\n",
        "\n",
        "max_length = 10 # Vectorizes the sample, You'll only consider the first max_length words in each sample.\n",
        "\n",
        "results = np.zeros(shape=(len(samples), max_length, max(token_index.values()) + 1))\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "      index = token_index.get(word)\n",
        "      results[i, j, index] = 1.\n"
      ],
      "metadata": {
        "id": "9-kQfYqgB9lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Character-level one-hot encoding (toy example)\n",
        "\n",
        "import string\n",
        "\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "characters = string.printable # All printable ASCII characters\n",
        "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
        "\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character in enumerate(sample):\n",
        "    index = token_index.get(character)\n",
        "    results[i, j, index] = 1."
      ],
      "metadata": {
        "id": "Xur_Y1J6DHtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Keras for word-level one-hot encoding\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000) # Creates a tokenizer, configured to only take into account the 1.000 most common words\n",
        "tokenizer.fit_on_texts(samples) # Builds the word index\n",
        "\n",
        "sequences = tokenizer.text_to_sequences(samples) # Turn strings into lists of integer indices\n",
        "\n",
        "one_hot_results = tokenizer.text_to_matrix(samples, mode=\"binary\") # Get the one-hot binary representations\n",
        "\n",
        "word_index = tokenizer.word_index # How you can recover the word index that was computed\n",
        "print(\"Found %s unique tokens.\" % len(word_index))"
      ],
      "metadata": {
        "id": "bfJ0fly4Dp84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word-level one-hot encoding with hashing trick (toy example)\n",
        "\n",
        "samples = [\"The cat sat on the mat.\", \"The dog ate my homework.\"]\n",
        "\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = abs(hash(word)) % dimensionality\n",
        "    results[i, j, index] = 1."
      ],
      "metadata": {
        "id": "eQI5jhkoEuDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.2 Using word embeddings"
      ],
      "metadata": {
        "id": "1677AX0wFuGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiating an Embedding layer\n",
        "\n",
        "from keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(1000, 64)"
      ],
      "metadata": {
        "id": "P0pzbWEaFyfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the IMDB data for use with an Embedding layer\n",
        "\n",
        "from keras.datasets import IMDB\n",
        "from keras import preprocessing\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 20\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen) # Turn the lists of integers into a 2D integer tensor of shape (samples, maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "dS-LAoNzGDdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using an Embedding layer and classifier on the IMDB data\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen)) # Specifies the maximum input length to the Embedding layer then flatten the embedded inputs\n",
        "\n",
        "model.add(Flatten()) # Flattens the 3D tensor of embeddings into a 2D tensor of shape (samples, maxlen * 8)\n",
        "\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "id": "q7BWtSn6GkCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.3 Putting it all together: from raw text to word embeddings\n"
      ],
      "metadata": {
        "id": "msIh7fJ5Ha9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data link: https://mng.bz/0tIo"
      ],
      "metadata": {
        "id": "2KHftNkzHn1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing the labels of the raw IMDB data\n",
        "\n",
        "import os\n",
        "\n",
        "imdb_dir = \"../ac1Imdb\"\n",
        "train_dir = os.path.join(imdb_dir, \"train\")\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in [\"neg\", \"pos\"]:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    if fname[-4:] == \".txt\":\n",
        "      f = open(os.path.join(dir_name, fname))\n",
        "      texts.append(f.read())\n",
        "      f.close()\n",
        "      if label_type == \"neg\":\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)"
      ],
      "metadata": {
        "id": "5QeA3oItHiN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the text of the raw IMDB data\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100\n",
        "training_samples = 200\n",
        "validation_samples = 10000\n",
        "max_words = 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words = max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.text_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Found %s unique tokens.\" % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "labels = np.asarray(labels)\n",
        "print(\"Shape of data tensor: \", data.shape)\n",
        "print(\"Shape of label tensor: \", labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "metadata": {
        "id": "wozPpUmbJYzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe link: https://nlp.stanford.edu/projects/glove"
      ],
      "metadata": {
        "id": "_TKmS-BKKugK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsing the GloVe word-embeddings file\n",
        "\n",
        "glove_dir = \"../glove.6B\"\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, \"glove.6B.100d.txt\"))\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "  embeddings_index[word] = coefs\n",
        "\n",
        "f.close()\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "id": "SbLgfsSFKt_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the GloVe word-embedding matrix\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      embedding_matrix[i] = embedding_vector # words not found in the embedding index will be all zeros."
      ],
      "metadata": {
        "id": "Y8SQwMPJKsFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length = maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "_PbqdSn6L4co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading pretrained word embeddings into the Embedding layer\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "metadata": {
        "id": "CTNqJG0sMKlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation\n",
        "\n",
        "model.compile(optimizer = \"rmsprop\",\n",
        "              loss = \"binary_crossentropy\",\n",
        "              metrics = [\"acc\"])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "model.save_weights(\"pre_trained_glove_model.h5\")"
      ],
      "metadata": {
        "id": "Wi2T0QBsMv3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the results\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history[\"acc\"]\n",
        "val_acc = history.history[\"val_acc\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tULqVwSPNEgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the same model without pretrained word embeddings\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "v1Y0soMbNwq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing the data of the test set\n",
        "\n",
        "test_dir = os.path.join(imdb_dir, \"test\")\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in [\"neg\", \"pos\"]:\n",
        "  dir_name = os.path.join(test_dir, label_type)\n",
        "  for fname in sorted(os.listdir(dir_name)):\n",
        "    if fname[-4:] == \".txt\":\n",
        "      f = open(os.path.join(dir_name, fname))\n",
        "      text.append(f.read())\n",
        "      f.close()\n",
        "      if label_type == \"neg\":\n",
        "        labels.append(0)\n",
        "      else:\n",
        "        labels.append(1)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "metadata": {
        "id": "WW6zeA39OYK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the test set\n",
        "model.load_weights(\"pre_trained_glove_model.h5\")\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "rgkObJC-PiRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.2 Understanding recurrent neural networks"
      ],
      "metadata": {
        "id": "QNf-JWcEPqe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudocode RNN\n",
        "state_t = 0 # the state at t\n",
        "for input_t in input_sequence: # iterates over sequence elements\n",
        "  output_t = f(input_t, state_t)\n",
        "  state_t = output_t # the previous output becomes the state for the next iteration."
      ],
      "metadata": {
        "id": "RpFRpmIpPt3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More detailed pseudocode for the RNN\n",
        "state_t = 0\n",
        "for input_t in input_sequences:\n",
        "  output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
        "  state_t = output_t"
      ],
      "metadata": {
        "id": "5HQVa4pnP_r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy implementation of a simple RNN\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "timesteps = 100 # Number of timesteps in the input sequence\n",
        "input_features = 32 # Dimensionality of the input feature space\n",
        "output_features = 64 # Dimensionality of the output feature space\n",
        "\n",
        "inputs = np.random.random((timesteps, input_features)) # Input data: random noise for the sake of the example\n",
        "\n",
        "state_t = np.zeros((output_features,)) # Initial state: an all-zero vector\n",
        "\n",
        "W = np.random.random((output_features, input_features))\n",
        "U = np.random.random((output_features, output_features))\n",
        "b = np.random.random((output_features,))\n",
        "\n",
        "successive_outputs = []\n",
        "for input_t in inputs: # input_t is a vector of shape (input_features,)\n",
        "  output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b) # Combines the input with the current state (the previous output) to obtain the current output\n",
        "\n",
        "  successive_outputs.append(output_t) # stores this output in a list\n",
        "\n",
        "  state_t = output_t # Updates the state of the network for the next timestep\n",
        "\n",
        "final_output_sequence = np.concatenate(successive_outputs, axis=0) # The final output is a 2D tensor of shape (timesteps, output_features)\n"
      ],
      "metadata": {
        "id": "gYDDw2lrQNQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.1 A recurrent layer in Keras"
      ],
      "metadata": {
        "id": "GhuUaPX7R4vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.summary()\n",
        "# Returns only the output at the last timestep"
      ],
      "metadata": {
        "id": "WLV5vaY9R9M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns the full state sequence\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7dNtMtjySVDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You have to get all of the intermediate layers to return full sequence of outputs\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32, return_sequences=True))\n",
        "model.add(SimpleRNN(32))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qJ4tcDYhSjW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the IMDB data\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 500\n",
        "batch_size = 32\n",
        "\n",
        "print(\"Loading data...\")\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(input_train), \"train_sequences\")\n",
        "print(len(input_test), \"test_sequences\")\n",
        "\n",
        "print(\"Pad sequences (samples x time)\")\n",
        "input_train = sequences.pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = sequences.pad_sequences(input_test, maxlen=maxlen)\n",
        "print(\"input_train.shape:\", input_train.shape)\n",
        "print(\"input_test.shape:\", input_test.shape)"
      ],
      "metadata": {
        "id": "1LqUE0A_TYbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model with Embedding and SimpleRNN layers\n",
        "\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "history = model.fit(input_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "id": "B7Gtihp_T_UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting results\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history[\"acc\"]\n",
        "val_acc = history.history[\"val_acc\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vJ8DfvYDUcyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.2 Understanding the LSTM and GRU layers"
      ],
      "metadata": {
        "id": "23lW21prU7i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudocode details of the LSTM architecture\n",
        "\n",
        "output_t = activation(dot(state_t, Uo) + dot(input_t, Wo) + dot(C_t, Vo) + bo)\n",
        "\n",
        "i_t = activation(dot(state_t, Ui) + dot(input_t, Wi) + bi)\n",
        "f_t = activation(dot(state_t, Uf) + dot(input_t, Wf) + bf)\n",
        "k_t = activation(dot(state_t, Uk) + dot(input_t, Wk) + bk)\n",
        "\n",
        "# Pseudocode details of the LSTM architecture 2\n",
        "c_t+1 = i_t * k_t * c_t * f_t"
      ],
      "metadata": {
        "id": "YK7uwrQEVBGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2.3 A concrete LSTM example in Keras"
      ],
      "metadata": {
        "id": "4UzIXupUVwMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the LSTM layer in Keras\n",
        "\n",
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"acc\"])\n",
        "\n",
        "history = model.fit(input_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "Z8ZxWY7YVzq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.3 Advanced use of recurrent neural networks"
      ],
      "metadata": {
        "id": "9z4Z8hrqWI93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.1 A temperature-forecasting problem\n",
        "\n",
        "Data link: https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip"
      ],
      "metadata": {
        "id": "62LBzs9kWM_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspecting the data of the Jena weather dataset\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir = \"../jena_climate\"\n",
        "fname = os.path.join(data_dir, \"jena_climate_2009_2016.csv\")\n",
        "\n",
        "f = open(fname)\n",
        "data = f.read()\n",
        "f.close()\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "header = lines[0].split(\",\")\n",
        "lines = lines[1:]\n",
        "\n",
        "print(header)\n",
        "print(len(lines))"
      ],
      "metadata": {
        "id": "DYePWGFdXAJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parsing the data\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "float_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "  values = [float(x) for x in line.split(\",\")[1:]]\n",
        "  float_data[i, :] = values"
      ],
      "metadata": {
        "id": "hFa062lDXlgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the temperature timeseries\n",
        "\n",
        "from matplotlib.pyplot as plt\n",
        "\n",
        "temp = float_data[:, 1]\n",
        "plt.plot(range(len(temp)), temp)\n",
        "\n",
        "plt.plot(range(1440), temp[:1440])"
      ],
      "metadata": {
        "id": "VBTRGrjcXz18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.2 Preparing the data"
      ],
      "metadata": {
        "id": "fdTVY0GzYKN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing the data\n",
        "\n",
        "mean = float_data[:200000].mean(axis=0)\n",
        "float_data -= mean\n",
        "std = float_data[:200000].std(axis=0)\n",
        "float_data /= std"
      ],
      "metadata": {
        "id": "hjRwt0kmYMda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator yielding timeseries samples and their targets\n",
        "\n",
        "def generator(data, lookback, delay, min_index, max_index, shuffle=False, batch_size=128, step=6):\n",
        "\n",
        "  if max_index is None:\n",
        "    max_index = len(data) - delay - 1\n",
        "  i = min_index + lookback\n",
        "  while 1:\n",
        "    if shuffle:\n",
        "      rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
        "    else:\n",
        "      if i + batch_size >= max_index:\n",
        "        i = min_index + lookback\n",
        "      rows = np.arange(i, min(i + batch_size, max_index))\n",
        "      i += len(rows)\n",
        "\n",
        "    samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
        "    targets = np.zeros((len(rows),))\n",
        "    for j, row in enumerate(rows):\n",
        "      indices = range(rows[j] - lookback, rows[j], step)\n",
        "      samples[j] = data[indices]\n",
        "      targets[j] = data[rows(j) + delay][1]\n",
        "\n",
        "    yield samples, targets"
      ],
      "metadata": {
        "id": "loaazDHkYY-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the training, validation and test generators\n",
        "\n",
        "lookback = 1440\n",
        "step = 6\n",
        "delay = 144\n",
        "batch_size = 128\n",
        "\n",
        "train_gen = generator(float_data,\n",
        "                      lookback=lookback,\n",
        "                      delay=delay,\n",
        "                      min_index=0,\n",
        "                      max_index=200000,\n",
        "                      shuffle=True,\n",
        "                      step=step,\n",
        "                      batch_size=batch_size)\n",
        "\n",
        "val_gen = generator(float_data,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=200001,\n",
        "                    max_index=300000,\n",
        "                    step=step,\n",
        "                    batch_size=batch_size)\n",
        "\n",
        "test_gen = generator(float_data,\n",
        "                     lookback=lookback,\n",
        "                     delay=delay,\n",
        "                     min_index=300001,\n",
        "                     max_index=None,\n",
        "                     step=step,\n",
        "                     batch_size=batch_size)\n",
        "\n",
        "val_steps = (300000 - 200001 - lookback)\n",
        "\n",
        "test_steps = (len(float_data) - 300001 - lookback)"
      ],
      "metadata": {
        "id": "bRr8GfstZTuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.3 A common-sense, non-machine-learning baseline"
      ],
      "metadata": {
        "id": "xPv92zqgaCFn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the common-sense baseline MAE\n",
        "\n",
        "def evaluate_naive_method():\n",
        "  batch_maes = []\n",
        "  for step in range(val_steps):\n",
        "    samples, targets = next(val_gen)\n",
        "    preds = samples[:, -1, 1]\n",
        "    mae = np.mean(np.abs(preds - targets))\n",
        "    batch_maes.append(mae)\n",
        "  print(np.mean(batch_maes))\n",
        "\n",
        "evaluate_naive_method()\n",
        "\n",
        "# Converting the MAE back to a Celsius error\n",
        "celsius_mae = 0.29 * std[1]"
      ],
      "metadata": {
        "id": "VpllNn1EaFwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.4 A basic machine-learning approach"
      ],
      "metadata": {
        "id": "mdTW1RKSaab0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluating a densely connected model\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Flatten(input_shape=(lookback // step, float_data√ßshape[-1])))\n",
        "model.add(layers.Dense(32, activation=\"relu\"))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss=\"mae\")\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "58Jp0Tutaljg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting results\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XmoLAUlka8tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.5 A first recurrent baseline"
      ],
      "metadata": {
        "id": "ARXZ8zHhbbjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluating a GRU-based model\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss=\"mae\")\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "1tVuf31bbeKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.6 Using recurrent dropout to fight overfitting"
      ],
      "metadata": {
        "id": "ljL9OOqUb8Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluating a dropout-regularized GRU-based model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "                     dropout=0.2,\n",
        "                     recurrent_dropout=0.2,\n",
        "                     input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss=\"mae\")\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "D8HAvcUecBNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.7 Stacking recurrent layers"
      ],
      "metadata": {
        "id": "6fZnjRwscZ0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "                     dropout=0.1,\n",
        "                     recurrent_dropout=0.5,\n",
        "                     return_sequences=True\n",
        "                     input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.GRU(64,\n",
        "                     activation=\"relu\"\n",
        "                     dropout=0.1,\n",
        "                     recurrent_dropout=0.5))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss=\"mae\")\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "Fk9U-kDDcdcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.8 Using Bidirectional RNNs"
      ],
      "metadata": {
        "id": "Hx7w7zwBeDq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluating an LSTM using reversed sequences\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 500\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.loab_data(num_words=max_features)\n",
        "\n",
        "x_train = [x[::-1] for x in x_train]\n",
        "x_test = [x[::-1] for x in x_test]\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(layers.LSTM(32))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit_generator(x_train, y_train,\n",
        "                              epochs=10,\n",
        "                              batch_size=128,\n",
        "                              validation_split=0.2)"
      ],
      "metadata": {
        "id": "b5f6OPdreGTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluating a bidirectional LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "id": "bumEX52Fe86W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a bidirectional GRU\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Bidirectional(layers.GRU(32), input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss=\"mae\")\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "yaeNrPFkfMBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.4 Sequence processing with convnets"
      ],
      "metadata": {
        "id": "R7grNg4ifeIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4.3 Implementing a 1D convnet"
      ],
      "metadata": {
        "id": "o2y2iDxVfiGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the IMDB data\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000\n",
        "max_len = 500\n",
        "\n",
        "print(\"Loading data...\")\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), \"train sequences\")\n",
        "print(len(x_test), \"test sequences\")\n",
        "\n",
        "print(\"Pad sequences (samples x time)\")\n",
        "x_train = sequences.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequences.pad_sequences(x_test, maxlen=maxlen)\n",
        "print(\"x_train.shape:\", x_train.shape)\n",
        "print(\"x_test.shape:\", x_test.shape)"
      ],
      "metadata": {
        "id": "RnntWsc6fzPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluating a simple 1D convnet on the IMDB data\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(layers.Conv1D(32, 7, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation=\"relu\"))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=1e-4), loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "id": "qHvT2eu9gwQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4.4 Combining CNNs and RNNs to process long sequences"
      ],
      "metadata": {
        "id": "W8cx6d-PhOy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluating a simple 1D convnet on the jena data\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Conv1D(32, 5, activation=\"relu\", input_shpae=(None, float.data.shape[-1])))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation=\"relu\"))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=1e-4), loss=\"mae\")\n",
        "history = model.fit(train_gen,\n",
        "                    steps_per_epoch=500,\n",
        "                    epochs=20,\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "_KOo3K2chXNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing higher-resolution data generators for the Jena dataset\n",
        "\n",
        "lookback = 720\n",
        "step = 3\n",
        "delay = 144\n",
        "\n",
        "train_gen = generator(float_data,\n",
        "                      lookback=lookback,\n",
        "                      delay=delay,\n",
        "                      min_index=0,\n",
        "                      max_index=200000,\n",
        "                      shuffle=True,\n",
        "                      step=step)\n",
        "\n",
        "val_gen = generator(float_data,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=200001,\n",
        "                    max_index=300000,\n",
        "                    step=step)\n",
        "\n",
        "test_gen = generator(float_data,\n",
        "                     lookback=lookback,\n",
        "                     delay=delay,\n",
        "                     min_index=300001,\n",
        "                     max_index=None,\n",
        "                     step=step)\n",
        "\n",
        "val_steps = (300000 - 200001 - lookback) // 128\n",
        "\n",
        "test_steps = (len(float_data) - 300001 - lookback) // 128"
      ],
      "metadata": {
        "id": "bMpUa8d6h1v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model combining a 1D convolutional base and a GRU layer\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Conv1D(32, 5, activation=\"relu\", input_shpae=(None, float.data.shape[-1])))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation=\"relu\"))\n",
        "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss=\"mae\")\n",
        "history = model.fit(train_gen,\n",
        "                    steps_per_epoch=500,\n",
        "                    epochs=20,\n",
        "                    validation_data=val_gen,\n",
        "                    validation_steps=val_steps)"
      ],
      "metadata": {
        "id": "Ou1bxRA7iI3U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}