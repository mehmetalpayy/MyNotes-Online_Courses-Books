{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 8.1 Text generation with LSTM"
      ],
      "metadata": {
        "id": "4YLzQ1fgSjTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1.3 The importance of the sampling strategy"
      ],
      "metadata": {
        "id": "txE3OezZV2a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reweighting a probability distribution to a different temperature\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def reweight_distribution(original_distribution, temperature=0.5): # original_distribution is a 1D Numpy array of probability values that must sum to 1.\n",
        "  distribution = np.log(original_distribution) / temperature\n",
        "  distribution = np.exp(distribution)\n",
        "  return distribution / np.sum(distribution) # Returns a reweighted version of the original distribution. The sum of the distribution may no longer be 1"
      ],
      "metadata": {
        "id": "i4aVFY6bV6qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.1.4 Implementing character-level LSTM text generation"
      ],
      "metadata": {
        "id": "RqnnsJWhWf-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and parsing the initial text file\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "path = keras.utils.get_file(\n",
        "    \"nietzsche.txt\",\n",
        "    origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
        ")\n",
        "text = open(path).read().lower()\n",
        "print(\"Corpus length:\", len(text))"
      ],
      "metadata": {
        "id": "SOn_q6Q0Wjtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing sequences of characters\n",
        "\n",
        "maxlen = 60 # You'll extract sequences of 60 characters\n",
        "step = 3 # You'll sample a new sequence every three characters\n",
        "sentences = [] # Holds the extracted sequences\n",
        "next_chars = [] # Holds the targets (the follow-up characters)\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "  sentences.append(text[i: i+maxlen])\n",
        "  next_chars.append(text[i + maxlen])\n",
        "\n",
        "print(\"Number of sequences:\", len(sentences))\n",
        "\n",
        "chars = sorted(list(set(text))) # List of unique characters in the corpus\n",
        "print(\"Unique characters:\", len(chars))\n",
        "char_indices = dict((char, chars.index(char)) for char in chars) # Dictionary that maps unique characters to their index in the list \"chars\"\n",
        "\n",
        "print(\"Vectorization...\")\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "  for t, char in enumerate(sentence):\n",
        "    x[i, t, char_indices[char]] = 1\n",
        "  y[i, char_indices[next_chars[i]]] = 1"
      ],
      "metadata": {
        "id": "SEb1m9bJW5tN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-layer LSTM model for next-character prediction\n",
        "\n",
        "from keras import layers\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(llen(chars), activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "hH0EymIfYDId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model compilation configuration\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"
      ],
      "metadata": {
        "id": "MwTQNQw1YSIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to sample the next character given the model's predictions\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "  preds = np.asarray(preds).astype(\"float64\")\n",
        "  preds = np.log(preds) / temperature\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "metadata": {
        "id": "Z4KC671VYc9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text-generation loop\n",
        "\n",
        "import random\n",
        "import sys\n",
        "\n",
        "for epoch in range(1, 60):\n",
        "  print(\"epoch\", epoch)\n",
        "  model.fit(x, y, batch_size=128, epochs=1) # Fits the model for one iteration on the data\n",
        "  start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "  generated_text = text[start_index: start_index + maxlen]\n",
        "  print('----Generating with seed: \"' + generated_text + '\"')\n",
        "\n",
        "  for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
        "    print(\"---------temperature:\", temperature)\n",
        "    sys.stdout.write(generated_text)\n",
        "\n",
        "  for i in range(400):\n",
        "    samples = np.zeros((1, maxlen, len(chars)))\n",
        "    for t, char in enumerate(generated_text):\n",
        "      sampled[0, t, char_index[char]] = 1.\n",
        "\n",
        "    preds = model.predict(sampled, verbose=0)[0]\n",
        "    next_index = sample(preds, temperature)\n",
        "    next_char = chars[next_index]\n",
        "\n",
        "    generated_text += next_char\n",
        "    generated_text = generated_text[1:]\n",
        "\n",
        "    sys.stdout.write(next_char)"
      ],
      "metadata": {
        "id": "R5bp7U8_YwbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.2 DeepDream"
      ],
      "metadata": {
        "id": "Pfg78FQXZz7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.2.1 Implementing DeepDream in Keras"
      ],
      "metadata": {
        "id": "eUqnBSvEZ122"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pretrained Inception V3 model\n",
        "\n",
        "from keras.applications import inception_v3\n",
        "from keras import backend as K\n",
        "\n",
        "K.set_learning_phase(0) # You won't be training the model, so this command disables all training-specific operations\n",
        "\n",
        "model = inception_v3.InceptionV3(weights=\"imagenet\",\n",
        "                                 include_top=False)"
      ],
      "metadata": {
        "id": "wxGqiAPeZ6DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the DeepDream configuration\n",
        "\n",
        "layer_contributions = {\n",
        "    \"mixed2\": 0.2,\n",
        "    \"midex3\": 3.,\n",
        "    \"mixed4\": 2.,\n",
        "    \"mixed5\": 1.5\n",
        "}"
      ],
      "metadata": {
        "id": "_SU5ufPRaO_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the loss to be maximized\n",
        "\n",
        "layer_dict = dict([(layer.name layer) for layer in model.layers]) # Creates a dictionary that maps layer names to layer instances\n",
        "\n",
        "loss = K.variable(0.) # You'll define the loss by adding layer contributions to this scalar variable\n",
        "for layer_name in layer_contributions:\n",
        "  coeff = layer_contributions[layer_name]\n",
        "  activation = layer_dict[layer_name].output # Retrieves the layer's output\n",
        "\n",
        "  scaling = K.prod(K.cast(K.shape(activation), \"float32\"))\n",
        "  loss += coeff * K.sum(K.square(activation[:, 2: -2, 2: -2, :])) / scaling"
      ],
      "metadata": {
        "id": "rhhjVk8iaaQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient-ascent process\n",
        "\n",
        "dream = model.input # This tensor holds the generated image: the dream\n",
        "grads = K.gradients(loss, dream)[0] # Computes the gradients of the dream with regard to the loss\n",
        "grads /= K.maximum(K.mean(K.abs(grads)), 1e-7) # Normalizes the gradients (important trick)\n",
        "\n",
        "outputs = [loss, grads]\n",
        "fetch_loss_and_grads = K.function([dream], outputs)\n",
        "\n",
        "def eval_loss_and_grads(X):\n",
        "  outs = fetch_loss_and_grads([x])\n",
        "  loss_value = outs[0]\n",
        "  grad_values = outs[1]\n",
        "  return loss_value, grad_values\n",
        "\n",
        "def gradient_ascent(x, iterations, step, max_loss=None):\n",
        "  for i in range(iterations):\n",
        "    loss_value, grad_values = eval_loss_and_grads(x)\n",
        "    if max_loss is not None and loss_value > max_loss:\n",
        "      break\n",
        "    print(\"...Loss value at\", i, \":\", loss_value)\n",
        "    x += step * grad_values\n",
        "  return x"
      ],
      "metadata": {
        "id": "hYr0LSVubECG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running gradient ascent over different successive scales\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "step = 0.01 # Gradient ascent step size\n",
        "num_octave = 3 # Number of scales at which to run gradient ascent\n",
        "octave_scale = 1.4 # Size ratio between scales\n",
        "iterations = 20 # Number of ascent steps to run at each scale\n",
        "\n",
        "max_loss = 10. # If the loss grows larger than 10, you'll interrupt the gradient-ascent process to avoid ugly artifacts\n",
        "\n",
        "base_image_path = \"...\"  # Fill this with the path to the image you want to use\n",
        "\n",
        "img = preprocess_image(base_image_path) # Loads the base image into a Numpy array (function is defined in listing 8.13)\n",
        "original_shape = img.shape[1:3]\n",
        "successive_shapes = [original_shape]\n",
        "for i in range(1, num_octave):\n",
        "  shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
        "  successive_shapes.append(shape)\n",
        "\n",
        "successive_shapes = successive_shapes[::-1] # Reverses the list of shapes so they're in increasing order\n",
        "\n",
        "original_img = np.copy(img)\n",
        "shrunk_original_img = resize_img(img, successive_shapes[0])\n",
        "\n",
        "for shape in successive_shapes:\n",
        "  print(\"Processing image shape\", shape)\n",
        "  img = resize_img(img, shape) # Scales up the dream image\n",
        "  img = gradient_ascent(img, iterations=iterations, step=step, max_loss=max_loss)\n",
        "\n",
        "  upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n",
        "  same_size_original = resize_img(original_img, shape) # Computes the high-quality version of the original image at this size\n",
        "  lost_detail = same_size_original - upscaled_shrunk_original_img # The difference between the two is the detail that was lost when scaling up\n",
        "\n",
        "  img += lost_detail # Reinjects lost detail into the dream\n",
        "  shrunk_original_img = resize_img(original_img, shape)\n",
        "  save_img(img, fname=\"dream_at_scale_\" + str(shape) + \".png\")\n",
        "\n",
        "save_img(img, fname=\"final_dream.png\")"
      ],
      "metadata": {
        "id": "FCOULWUNcEpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auxiliary functions\n",
        "\n",
        "import scipy\n",
        "from keras.preprocessing import image\n",
        "\n",
        "def resize_img(img, size):\n",
        "  img = np.copy(img)\n",
        "  factors = (1, float(size[0]) / img.shape[1], float(size[1]) / img.shape[2], 1)\n",
        "\n",
        "  return scipy.ndimage.zoom(img, factors, order=1)\n",
        "\n",
        "def save_img(img, fname):\n",
        "  pil_img = deproces_image(np.copy(img))\n",
        "  scipy.misc.imsave(fname, pil_img)\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "  img = image.load_img(image_path)\n",
        "  img = image.img_to_array(img)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  img = inception_v3.preprocess_input(img)\n",
        "  return img\n",
        "\n",
        "def deprocess_img(x):\n",
        "  if K.image_data_format() == \"channels_first\":\n",
        "    x = x.reshape((3, x.shape[2], x.shape[3]))\n",
        "    x = x.transpose((1, 2, 0))\n",
        "\n",
        "  else:\n",
        "    x = x.reshape((x.shape[1], x.shape[2], 3))\n",
        "\n",
        "  x /= 2.\n",
        "  x += 0.5\n",
        "  x *= 255.\n",
        "  x = np.clip(x, 0, 255).astype(\"uint8\")\n",
        "  return x"
      ],
      "metadata": {
        "id": "cxN_b_kPeMQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.3 Neural style transfer"
      ],
      "metadata": {
        "id": "I_YdKRHlfQmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.3.3 Neural style transfer in Keras"
      ],
      "metadata": {
        "id": "SJK65gg4fS2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining initial variables\n",
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "target_image_path = \"img/portrait.jpg\" # Path to the image you want to transform\n",
        "style_reference_image_path = \"img/transfer_style_reference.jpg\" # Path to the style image\n",
        "\n",
        "width, height = load_img(targeT_image_path).size\n",
        "img_height = 400\n",
        "img_width = int(width * img_height / height)"
      ],
      "metadata": {
        "id": "A0k4oaDDfV2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Auxiliary functions\n",
        "\n",
        "import numpy as np\n",
        "from keras.apllications import vgg19\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "  img = load_img(image_path, target_size=(img_height, img_width))\n",
        "  img = img_to_array(img)\n",
        "  img = np.expand_dims(img, axis=0)\n",
        "  img = vgg19.preprocess_input(img)\n",
        "  return img\n",
        "\n",
        "def deprocess_image(x):\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  x = x[:, :, ::-1] # Converts images from \"BGR\" to \"RGB\". This is also part of the reversal of vgg19.preprocess_input.\n",
        "  x = np.clip(x, 0, 255).astype(\"uint8\")\n",
        "  return x"
      ],
      "metadata": {
        "id": "Hl6m4aCMgrwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pretrained VGG19 network and applying it to the three images\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "target_image = K.constant(preprocess_image(target_image_path))\n",
        "style_reference_image = K.constant(preprocess_image(style_reference_image_path))\n",
        "combination_image = K.placeholder((1, img_height, img_width, 3)) # Placeholder that will contain the generated image\n",
        "\n",
        "input_tensor = K.concatenate([target_image, style_reference_image, combination_image], axis=0) # Combines the three images in a single batch\n",
        "\n",
        "model = vgg19.VGG19(input_tensor=input_tensor,\n",
        "                    weights=\"imagenet\",\n",
        "                    include_top=False)\n",
        "print(\"Model loaded.\")"
      ],
      "metadata": {
        "id": "u434dnRkha2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Content loss\n",
        "\n",
        "def content_loss(base, combination):\n",
        "  return K.sum(K.square(combination - base))\n",
        "\n",
        "# Style loss\n",
        "\n",
        "def gram_matrix(x):\n",
        "  features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
        "  gram = K.dot(features, K.transpose(features))\n",
        "  return gram\n",
        "\n",
        "def style_loss(style, combination):\n",
        "  S = gram_matrix(style)\n",
        "  C = gram_matrix(combination)\n",
        "  channels = 3\n",
        "  size = img_height * img_width\n",
        "  return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))"
      ],
      "metadata": {
        "id": "bdnUMN-0iAOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total variation loss\n",
        "\n",
        "def total_variation_loss(x):\n",
        "  a = K.square(\n",
        "      x[:, :img_height - 1, :img_width - 1, :] - x[:, 1:, :img_width - 1, :]\n",
        "  )\n",
        "  b = K.square(\n",
        "      x[:, :img_height - 1, :img_width - 1, :] - x[:, :img_height - 1, 1:, :]\n",
        "  )\n",
        "  return K.sum(K.pow(a + b, 1.25))"
      ],
      "metadata": {
        "id": "RUdlg8IpijXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the final loss that you'll minimize\n",
        "\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "content_layer = \"block5_conv2\"\n",
        "style_layers = [\"block1_conv1\",\n",
        "                \"block2_conv1\",\n",
        "                \"block3_conv1\",\n",
        "                \"block4_conv1\",\n",
        "                \"block5_conv1\"]\n",
        "total_variation_weight = 1e-4\n",
        "style_weight = 1.\n",
        "content_weight = 0.025\n",
        "loss = K.variable(0.)\n",
        "layer_features = outputs_dict[content_layer]\n",
        "target_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "loss += content_weight * content_loss(target_image_features, combination_features)\n",
        "\n",
        "for layer_name in style_layers:\n",
        "  layer_features = outputs_dict[layer_name]\n",
        "  style_reference_features = layer_features[1, :, :, :]\n",
        "  combination_features = layer_features[2, :, :, :]\n",
        "  sl = style_loss(style_reference_features, combination_features)\n",
        "  loss += (style_weight / len(style_layers)) * sl\n",
        "\n",
        "loss += total_variation_weight * total_variation_loss(combination_image)"
      ],
      "metadata": {
        "id": "DXoouPjUjGFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the gradient-descent process\n",
        "\n",
        "grads = K.gradients(loss, combination_image)[0] # Gets the gradients of the generated image with regard to the loss\n",
        "\n",
        "fetch_loss_and_grads = K.function([combination_image], [loss, grads]) # Function to fetch the values of the current loss and the current gradients\n",
        "\n",
        "class Evaluator(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.loss_value = None\n",
        "    self.grads_values = None\n",
        "\n",
        "  def loss(self, x):\n",
        "    assert self.loss_value is None\n",
        "    x = x.reshape((1, img_height, img_width, 3))\n",
        "    outs = fetch_loss_and_grads(x)\n",
        "    loss_value = outs[0]\n",
        "    grad_values = outs[1].flatten().astype(\"float64\")\n",
        "    self.loss_value = loss_value\n",
        "    self.grad_values = grad_values\n",
        "    return self.loss_value\n",
        "\n",
        "  def grads(self, x):\n",
        "    assert self.loss_value is not None\n",
        "    grad_values = np.copy()\n",
        "    self.loss_value = None\n",
        "    self.grad_values = None\n",
        "    return grad_values\n",
        "\n",
        "evaluator = Evaluator()"
      ],
      "metadata": {
        "id": "QjyaHfPdxoHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Style-transfer loop\n",
        "\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "from scipy.misc import imsave\n",
        "import time\n",
        "\n",
        "result_prefix = \"my_result\"\n",
        "iterations = 20\n",
        "\n",
        "x = preprocess_image(target_image_path) # This is the initial state: the target image.\n",
        "x = x.flatten()\n",
        "\n",
        "for i in range(iterations):\n",
        "  print(\"Start of iteration\", i)\n",
        "  start_time = time.time()\n",
        "  x, min_val, info = fmin_l_bfgs_b(evaluator.loss,\n",
        "                                   x,\n",
        "                                   fprime=evaluator.grads,\n",
        "                                   maxfun=20)\n",
        "  print(\"Current loss value:\", min_val)\n",
        "  img = x.copy().reshape((img_height, img_width, 3))\n",
        "  img = deprocess_img(img)\n",
        "  fname = result_prefix + \"_at_iteration_%d.png\" % i\n",
        "  imsave(fname, img)\n",
        "  print(\"Image saved as\", fname)\n",
        "  end_time = time.time()\n",
        "  print(\"Iteration %d completed in %ds\" % (i, end_time - start_time))"
      ],
      "metadata": {
        "id": "-7iTSuc_z7s_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.4 Generating images with variational autoencoders"
      ],
      "metadata": {
        "id": "SPIsQ9nE1ker"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.4.3 Variational autoencoders"
      ],
      "metadata": {
        "id": "btwiLLTU1ouy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# VAE encoder network\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "img_shape = (28, 28, 1)\n",
        "batch_size = 16\n",
        "latent_dim = 2 # Dimensionality of the latent space: a 2D plane\n",
        "\n",
        "input_img = keras.Input(shape=img_shape)\n",
        "\n",
        "x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(input_img)\n",
        "x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", strides=(2, 2))(x)\n",
        "x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
        "shape_before_flattening = K.int_shape(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "\n",
        "z_mean = layers.Dense(latent_dim)(x)\n",
        "z_log_var = layers.Dense(latent_dim)(x)"
      ],
      "metadata": {
        "id": "gRU-VVTi1uGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Latent-space-sampling function\n",
        "\n",
        "def sampling(args):\n",
        "  z_mean, z_log_var = args\n",
        "  epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=1.)\n",
        "  return z_mean + K.exp(z_log_var) * epsilon\n",
        "\n",
        "z = layers.Lambda(sampling)([z_mean, z_log_var])"
      ],
      "metadata": {
        "id": "Z19zsf4p2klL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VAE decoder network, mapping latent space points to images\n",
        "\n",
        "decoder_input = layers.Input(K.int_shape(z)[1:]) # Input where you'll feed z\n",
        "\n",
        "x = layers.Dense(np.prod(shape_before_flattening[1:]),\n",
        "                 activtion=\"relu\")(decoder_input)\n",
        "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
        "x = layers.Conv2DTranspose(32, 3,\n",
        "                           padding=\"same\",\n",
        "                           activation=\"relu\",\n",
        "                           strides=(2, 2))(x)\n",
        "x = layers.Conv2D(1, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
        "decoder = Model(decoder_input, x)\n",
        "z_decoded = decoder(z)"
      ],
      "metadata": {
        "id": "BrtApANH26Dq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom layer used to compute the VAE loss\n",
        "\n",
        "class CustomVariationalLayer(keras.layers.Layer):\n",
        "\n",
        "  def vae_loss(self, x, z_decoded):\n",
        "    x = K.flatten(x)\n",
        "    z_decoded = K.flatten(z_decoded)\n",
        "    xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
        "    kl_loss = -5e-4 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "    return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = inputs[0]\n",
        "    z_decoded = inputs[1]\n",
        "    loss = self.vae_loss(x, z_decoded)\n",
        "    self.add_loss(loss, inputs=inputs)\n",
        "    return x # You don't use this output, but the layer must return something"
      ],
      "metadata": {
        "id": "URcAORqO4LMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the VAE\n",
        "\n",
        "from keras.datasets import mnist\n",
        "\n",
        "vae = Model(input_img, x)\n",
        "vae.compile(optimizer=\"rmsprop\", loss=None)\n",
        "vae.summary()\n",
        "\n",
        "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype(\"float32\") / 255.\n",
        "x_train = x_train.reshape(x_train.shape + (1,))\n",
        "x_test = x_test.astype(\"float32\") / 255.\n",
        "x_test = x_test.reshape(x_test.shape + (1,))\n",
        "\n",
        "vae.fit(x=x_train, y=None,\n",
        "        shuffle=True,\n",
        "        epochs=10,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test, None))"
      ],
      "metadata": {
        "id": "fzK-Cvf872vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling a grid of points from the 2D latent space and decoding them to images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "n = 15\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
        "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
        "\n",
        "for i, yi in enumerate(grid_x):\n",
        "  for j, xi in enumerate(grid_y):\n",
        "    z_sample = np.array([[xi, yi]])\n",
        "    z_sample = np.tile(z_sample, batch_size).reshape(batch_size, 2)\n",
        "    x_decoded = decoder.predict(z_sample, batch_size=batch_size)\n",
        "    digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "    figure[i * digit_size: (i+1) * digit_size,\n",
        "           j * digit_size: (j+1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap=\"Greys_r\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jjhPvktP8cSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.5 Introduction to generative adversarial networks"
      ],
      "metadata": {
        "id": "VhiO8l0U9cZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.5.3 The generator"
      ],
      "metadata": {
        "id": "Eh5pdJDI9hH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GAN generator network\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "\n",
        "latent_dim = 32\n",
        "height = 32\n",
        "width = 32\n",
        "channels = 3\n",
        "\n",
        "generator_input = keras.Input(shaoe=(latent_dim,))\n",
        "\n",
        "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Reshape((16, 16, 128))(x)\n",
        "\n",
        "x = layers.Conv2D(256, 5, padding=\"same\")(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "x = layers.Conv2DTranspose(256, 5, strides=2, padding=\"same\")(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "x = layers.Conv2D(256, 5, padding=\"same\")(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(256, 5, padding=\"same\")(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "x = layers.Conv2D(channels, 7, activation=\"tanh\", padding=\"same\")(x)\n",
        "generator = keras.models.Model(generator_input, x)\n",
        "generator.summary()"
      ],
      "metadata": {
        "id": "WMk39EVF9ksp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.5.4 The discriminator"
      ],
      "metadata": {
        "id": "Qc9bJp0k-vH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The GAN discriminator network\n",
        "\n",
        "discriminator_input = layers.Input(shape=(height, width, channels))\n",
        "x = layers.Conv2D(128, 3)(discriminator_input)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(discriminator_input)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Conv2D(128, 4, strides=2)(discriminator_input)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "discriminator = keras.models.Model(discriminator_input, x)\n",
        "discriminator.summary()\n",
        "\n",
        "discriminator_optimizer = keras.optimizers.RMSprop(\n",
        "    lr = 0.0008,\n",
        "    clipvalue = 1.0,\n",
        "    decay = 1e-8\n",
        ")\n",
        "\n",
        "discriminator.compile(optimizer=discriminator_optimizer,\n",
        "                      loss=\"binary_crossentropy\")"
      ],
      "metadata": {
        "id": "a5H935QD_L9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.5.5 The adversarial network"
      ],
      "metadata": {
        "id": "fv6X05jHAHFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adversarial network\n",
        "\n",
        "discriminator.trainable = False\n",
        "\n",
        "gan_input = keras.Input(shape=(latent_dim,))\n",
        "gan_output = discriminator(generator(gan_input))\n",
        "gan = keras.models.Model(gan_input, gan_output)\n",
        "\n",
        "gan_optimizer = keras.optimizers.RMSprop(lr = 0.0004, clipvalue=1.0, decay=1e-8)\n",
        "gan.compile(optimizer=gan_optimizer, loss=\"binary_crossentropy\")"
      ],
      "metadata": {
        "id": "aIJPvI8ZAK2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.5.6 How to train your DCGAN"
      ],
      "metadata": {
        "id": "8cGh4q6AAnI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing GAN training\n",
        "\n",
        "import os\n",
        "from keras.preprocessing import image\n",
        "\n",
        "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
        "x_train = x_train[y_train.flatten() == 6] # Selects frog images (class 6)\n",
        "x_train = x_train.reshape(\n",
        "    (x_train.shape[0],) +\n",
        "    (height, width, channels)).astype(\"float32\") / 255.\n",
        "\n",
        "iterations = 10000\n",
        "batch_size = 20\n",
        "save_dir = \"your_dir\"\n",
        "\n",
        "start = 0\n",
        "for step in range(iterations):\n",
        "  random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) # Samples random points in the latent space\n",
        "  generated_images = generator.predict(random_latent_vectors) # Decodes them to fake images\n",
        "\n",
        "  stop = start + batch_size\n",
        "  real_images = x_train[start: stop]\n",
        "  combined_images = np.concatenate([generated_images, real_images]) # Combines them with real images\n",
        "  labels = np.concatenate([np.ones((batch_size, 1)),\n",
        "                           np.zeros((batch_size))])\n",
        "  labels += 0.05 * np.random.random(labels.shape) # Adds random noise to the labels\n",
        "  d_loss = discriminator.train_on_batch(combined_images, labels) # Trains the discriminator\n",
        "  random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) # Samples random points in the latent space\n",
        "  misleading_targets = np.zeros((batch_size, 1)) # Assembles labels that say \"these are all real images\"\n",
        "  a_loss = gan.train_on_batch(random_latent_vectors, misleading_vectors) # Trains the generator (via the gan model, where the discriminator weights are frozen)\n",
        "\n",
        "  start += batch_size\n",
        "  if start > len(x_train) - batch_size:\n",
        "    start = 0\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    gan.save_weights(\"gan.h5\")\n",
        "\n",
        "    print(\"discriminator loss:\", d_loss)\n",
        "    print(\"adversarial loss:\", a_loss)\n",
        "\n",
        "    img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
        "    img.save(os.path.join(save_dir, \"generated_frog\" + str(step) + \".png\"))\n",
        "    img = image.array_to_img(real_images[0] * 255., scale=False)\n",
        "    img.save(os.path.join(save_dir, \"real_fog\" + str(step) + \".png\"))"
      ],
      "metadata": {
        "id": "XiGqQtNIAp0Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}