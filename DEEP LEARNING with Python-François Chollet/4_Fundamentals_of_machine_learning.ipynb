{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating Machine Learning Models"
      ],
      "metadata": {
        "id": "WCdyLNbMllu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Hold-out Validation"
      ],
      "metadata": {
        "id": "hRvQuRQ9uK12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hold-out Validation\n",
        "\n",
        "num_validation_samples = 10000\n",
        "np.random.shuffle(data)\n",
        "validation_data = data[:num_validation_samples] # validation set\n",
        "data = data[num_validation_samples:] # training set\n",
        "\n",
        "training_data = data[:]\n",
        "\n",
        "model = get_model()\n",
        "model.train(training_data)\n",
        "validation_score = model.evaluate(validation_data)\n",
        "\n",
        "# At this point you can tune your model\n",
        "# retrain it, evaluate it, tune it again\n",
        "\n",
        "model = get_model()\n",
        "model.train(np.concatenate([training_data,\n",
        "                            validation_data]))\n",
        "test_score = model.evaluate(test_data)"
      ],
      "metadata": {
        "id": "Ogq2DVR5uOGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veriseti az ise bu problem oluşturacaktır ve farklı random shuffling değerleri çok farklı sonuçlar veriyorsa muhtemelen az verimiz var demektir."
      ],
      "metadata": {
        "id": "U9s9dHqFvVKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-Fold Validation"
      ],
      "metadata": {
        "id": "5iqAVpj0vDuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold cross-validation\n",
        "\n",
        "k = 4\n",
        "num_validation_samples = len(data) // k\n",
        "\n",
        "np.random.shuffle(data)\n",
        "\n",
        "validation_scores = []\n",
        "for fold in range(k):\n",
        "  validation_data = data[num_validation_samples * fold: num_validation_samples * (fold + 1)]\n",
        "  training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1):]\n",
        "\n",
        "  model = get_model()\n",
        "  model.train(training_data)\n",
        "  validation_score = model.evaluate(validation_data)\n",
        "  validation_scores.append(validation_score)\n",
        "\n",
        "validation_score = np.average(validation_scores)\n",
        "\n",
        "model = get_model()\n",
        "model.train(data)\n",
        "test_score = model.evaluate(test_data)"
      ],
      "metadata": {
        "id": "KrgOSihPzz3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Value normalization**\n",
        "\n",
        "Normalize each feature independently to have a mean of 0.\n",
        "\n",
        "Normaliza each feature independently to have a standard deviation of 1.\n"
      ],
      "metadata": {
        "id": "usSaDoK_2fS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Value normalization\n",
        "\n",
        "# Assuming x is a 2D data matrix of shape\n",
        "x -= x.mean(axis=0)\n",
        "x /= x.std(axis=0)"
      ],
      "metadata": {
        "id": "Ah1RtlIl0shM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reducing the network's size"
      ],
      "metadata": {
        "id": "83bjz6ej2vYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelin tüm parametrelerini beslemek için büyük networkler için büyük verisetleri gerekir bu yüzden de veriye bağlı küçük networkle başlamak ya da\n",
        "# modeli küçültmek overfitting gibi durumlara yakalanmamamız için önemlidir.\n",
        "\n",
        "# Original model\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation=\"relu\", input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation=\"relu\"))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# Version of the model with lower capacity\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(4, activation=\"relu\", input_shape=(10000,)))\n",
        "model.add(layers.Dense(4, activation=\"relu\"))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))"
      ],
      "metadata": {
        "id": "etoaLjJC5ncH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Version of the model with higher capacity\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(512, activation=\"relu\", input_shape=(10000,)))\n",
        "model.add(layers.Dense(512, activation=\"relu\"))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# Simpler models are less likely to overfit than complex ones."
      ],
      "metadata": {
        "id": "Vm9YGBp96SvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To mitigate overfitting, use L1, L2 regularization, L2 regularization = weight decay\n",
        "\n",
        "# Adding L2 weight regularization to the model\n",
        "\n",
        "from keras import regularizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                       activation=\"relu\", input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                       activation=\"relu\"))\n",
        "model.add(layers.Dense(16, activation=\"sigmoid\"))\n",
        "\n",
        "# l2(0.001) means every coefficient in the weight matrix of the layer will add 0.001 * weight_coefficient_value\n",
        "\n",
        "# As an alternative to L2 regularization\n",
        "\n",
        "regularizers.l1(0.001) # L1 regularization\n",
        "\n",
        "regularizers.l1_l2(l1=0.001, l2=0.001) # L1 and L2 regularization"
      ],
      "metadata": {
        "id": "WCWFTd_E7X3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding dropout\n",
        "# Dropout is one of the most effective and most commonly used regularization techniques for neural networks\n",
        "\n",
        "layer_output *= np.random.randint(0, high=2, size=layer_output.shape) # At training time, drops out %50 of the units in the output\n",
        "# At test time, we scale down the output by the dropout rate\n",
        "layer_output *= 0.5 # At test time\n",
        "\n",
        "# Adding dropout to the IMDB network\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation=\"relu\", input_shape=(10000,)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(16, activation=\"relu\"))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation=\"sigmoid\"))"
      ],
      "metadata": {
        "id": "JSVXa4Or8Vn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "41L15Ens_E9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}