# -*- coding: utf-8 -*-
"""00_tensorflow_fundamentals.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11nwQMvCGw-_NM7QUj7Eua-LsUc-PMoq9

# In this notebook, we are going to cover some of the most fundamental concepts of tensors using TensorFlow

## Introduction to Tensors
"""

# Import TensorFlow
import tensorflow as tf
print(tf.__version__)

# Create tensors with tf.constant()
scalar = tf.constant(7)
scalar

# Check the number of dimensions of a tensor (ndim stands for number of dimensions)
scalar.ndim

# Create a vector
vector = tf.constant([10, 10])
vector

# Check the dimension of our vector
vector.ndim

# Create a matrix (has more than 1 dimension)
matrix = tf.constant([[10, 7],
                      [7, 10]])
matrix

matrix.ndim

# Create another matrix
another_matrix = tf.constant([[10., 7.],
                              [3., 2.],
                              [8., 9.]], dtype = tf.float16) # specify the data type with dtype parameter
# float32 float16 ya da int32 int16 gibi tip değerleri arasındaki fark hafıza sınırıdır 16lı olanlar daha küçük hafıza kullanır. Ayrıca dtype parametresini kullanarak
# verileri ve depolamayı manipüle edebiliriz.
another_matrix

# What's the number dimensions of another_matrix?
another_matrix.ndim

# Let's create a tensor
tensor = tf.constant([[[1, 2, 3],
                       [4, 5, 6]],
                      [[7, 8, 9],
                       [10, 11, 12]],
                      [[13, 14, 15],
                       [16, 17, 18]]])
# scalar,vector,matrix ve tensor olusturduk ve hepsi farkli dimensiona sahipti fakat biz bunların hepsine tensor diyecegiz tum boyutlar birer tensor olarak adlandırılır
# (ndim) burdaki n yerine ne gelirse gelsin bir Tensor olur.
tensor

tensor.ndim

"""What we have created so far:
* Scalar: a single number
* Vector: a number with direction (e.g. wind speed and direction)
* Matrix: a 2-dimensional array of numbers
* Tensor: an n-dimensional array of numbers (when n can be any number, a 0-dimensional tensor is a scalar, a 1-dimensional tensor is a vector)

### Creating tensors with "tf.Variable"
"""

tf.Variable

# Create the same tensor with tf.Variable() as above, eger bir arrayin degistirebilir olmasini istiyorsak tf.Variable eger degistirilmez olsun diyorsak tf.constant
changeable_tensor = tf.Variable([10, 7])
unchangeable_tensor = tf.constant([10, 7])
changeable_tensor, unchangeable_tensor

# Let's try change one of the elements in our changeable tensor
changeable_tensor[0] = 7
changeable_tensor

# How about we try .assign()
changeable_tensor[0].assign(7)
changeable_tensor

# Let's try change our unchangeable tensor
unchangeable_tensor[0] = 7

unchangeable_tensor[0].assign(7)
unchangeable_tensor

"""**Note**: Rarely in practice will you need to decide whether to use tf.constant or tf.Variable to create tensors, as TensorFlow does this for you. However, if in doubt, use tf.constant and change it later if needed.

### Creating random tensors

Random tensors are tensors of some arbitrary size which contain random numbers.
"""

# Create two random (but the same) tensors
# Bu ornekte random numaralar almak istememize ve o tarz islem yapmamıza ragmen random_1 ve random_2 ayni degerleri aldılar
# bunun sebebi her ikisinin de seed(42) olarak ayarlanmasıdır.
random_1 = tf.random.Generator.from_seed(42) # set seed for reproducibility
random_1 = random_1.normal(shape = (3, 2))
random_2 = tf.random.Generator.from_seed(42)
random_2 = random_2.normal(shape=(3, 2))

# Are they equal?
random_1, random_2, random_1 == random_2

"""### Shuffle the order of elements in a tensor"""

# Shuffle a tensor (valuable for when you want to shuffle your data so the inherent order doesn't effect learning)
not_shuffled = tf.constant([[10, 7],
                            [3, 4],
                            [2, 5]])
# Shuffle our non_shuffled tensor
tf.random.shuffle(not_shuffled)

# Shuffle our non_shuffled tensor
# Eger bir fonksiyonun icinde seed kullanırsak bu operation oldugu icin farkli farkli sonuclar almaya devam ederiz.
tf.random.shuffle(not_shuffled, seed = 42)

"""It looks like if we want our shuffled tensors to be in the same order, we've got to use the global level random seed as well as the operation level random seed."""

# Eger global seed fonksiyonu kullanırsak her zaman ayni sonucu aliriz.
tf.random.set_seed(42)
tf.random.shuffle(not_shuffled, seed = 42)

# burda da ayni sonucu surekli uretir fakat kullandigimiz surume gore bunun mantigi degisebilir
tf.random.set_seed(42)
tf.random.shuffle(not_shuffled)

not_shuffled

"""### Other ways to make tensors"""

# Create a tensor of all ones
tf.ones([10, 7])

# Create a tensor of all zeroes
tf.zeros(shape=(3, 4))

"""### Turn NumPy arrays into tensors

The main difference between NumPy arrays and TensorFlow tensors is that tensors can be run on a GPU (much faster for numerical computing).
"""

# You can also turn NumPy arrays into tensors
import numpy as np
numpy_A = np.arange(1, 25, dtype = np.int32) # create a NumPy array between 1 and 25
numpy_A

# X = tf.constant(some_matrix) # capital for matrix or tensor
# y = tf.constant(vector) # non-capital for vector

A = tf.constant(numpy_A)
A

A = tf.constant(numpy_A, shape=(2, 3, 4))
A

A = tf.constant(numpy_A, shape=(3, 8))
A

"""### Getting information from tensors

When dealing with tensors you probably want to be aware of the following attributes:
* Shape: The length (number of elements) of each of the dimensions of a tensor.
* Rank: The number of tensor dimensions. A scalar has rank 0, a vector has rank 1, a matrix is rank 2, a tensor has rank n.
* Axis or dimension: A particular dimension of a tensor.
* Size: The total number of items in the tensor.
"""

# Create a rank 4 tensor (4 dimensions)
rank_4_tensor = tf.zeros(shape = [2, 3, 4, 5])
rank_4_tensor

rank_4_tensor[0]

rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor)

# Get various attributes of our tensor
print("Datatype of every element:", rank_4_tensor.dtype)
print("Number of dimensions (rank):", rank_4_tensor.ndim)
print("Shape of tensor:", rank_4_tensor.shape)
print("Elements along the 0 axis:", rank_4_tensor.shape[0])
print("Elements along the last axis:", rank_4_tensor.shape[-1])
print("Total number of elements in our tensor:", tf.size(rank_4_tensor))
print("Total number of elements in our tensor:", tf.size(rank_4_tensor).numpy())

"""## Indexing tensors
### Tensors can be indexed just like Python lists.
"""

# Get the first 2 elements of each dimension
rank_4_tensor[:2,:2,:2,:2]

# Get the first element from each dimension from each index except for the final one
rank_4_tensor[:1,:1,:1]

# Create a rank 2 tensor (2 dimensions)
rank_2_tensor = tf.constant([[10, 7],
                            [3, 4]])
rank_2_tensor.shape, rank_2_tensor.ndim

# Get the last item of each row of our rank 2 tensor
rank_2_tensor[:,-1]

# Add in extra dimension to our rank 2 tensor
rank_3_tensor = rank_2_tensor[...,tf.newaxis]
rank_3_tensor

# Alternative to tf.newaxis
tf.expand_dims(rank_2_tensor, axis = -1) # -1 means expand the final axis

# Expand the 0-axis
tf.expand_dims(rank_2_tensor, axis = 0) # expand the 0-axis

"""### Manipulating tensors (tensor operations)

**Basic operations**

`+`, `-`, `*`, `/`


"""

# You can add values to a tensor using the addition operator
tensor = tf.constant([[10, 7], [3, 4]])
tensor + 10

# Original tensor is unchanged
tensor

# Multiplication also works
tensor * 10

# Subtraction
tensor - 10

# We can use the tensorflow built-in function too
tf.multiply(tensor, 10)

"""**Matrix Multiplication**

In machine learning, matrix multiplication is one of the most common tensor operations.

There are two rules our tensors (or matrices) need to fulfill if we're going to matrix multiply them:

1. The inner dimensions must match
2. The resulting matrix has the shape of the outer dimensions
"""

# Matrix multiplication in tensorflow
print(tensor)
tf.matmul(tensor,tensor)

# Matrix multiplication with Python operator "@"
tensor @ tensor

# Element-wise operation *
tensor * tensor

tensor.shape

# Create a tensor (3, 2) tensor
X = tf.constant([[1, 2],
                 [3, 4],
                 [5, 6]])

# Create another (3, 2) tensor
Y = tf.constant([[7, 8],
                 [9, 10],
                 [11, 12]])
X, Y

# Try to matrix multiply tensors of same shape
X @ Y

# Let's change the shape of Y
tf.reshape(Y, shape=(2, 3))

# Try to multiply matrix X by reshaped Y
X @ tf.reshape(Y, shape=(2, 3))

tf.matmul(X, tf.reshape(Y, shape=(2, 3)))

# Try change the shape of X instead of Y
tf.matmul(tf.reshape(X, shape=(2, 3)), Y)

tf.reshape(X, shape=(2, 3)).shape, Y.shape

# Can do the same with transpose
X, tf.transpose(X), tf.reshape(X, shape=(2, 3))

#Try matrix multiplication with transpose rather than reshape
tf.matmul(tf.transpose(X), Y)

"""**The dot product**

Matrix multiplication is also referred to as the dot product

You can perform matrix multiplication using:
* `tf.matmul()`
* `tf.tensordot()`
"""

X, Y

# Perform the dot product on X and Y (requires X or Y to be transposed)
tf.tensordot(tf.transpose(X), Y, axes = 1)

# Perform matrix multiplication between X and Y (transposed)
tf.matmul(X, tf.transpose(Y))

# Perform matrix multiplication between X and Y (reshaped)
tf.matmul(X, tf.reshape(Y, shape = (2, 3)))

# Check the values of Y, reshape Y and transposed Y
print("Normal Y:")
print(Y, "\n")

print("Y reshaped to (2, 3):")
print(tf.reshape(Y, (2, 3)), "\n")

print("Y transpose:")
print(tf.transpose(Y))

tf.matmul(X, tf.transpose(Y))

"""Generally, when performing matrix multiplication on two tensors and one of the axes doesn't line up, you will transpose (rather than reshape) one of the tensors to satisfy the matrix multiplication rules.

### Changing the datatype of a tensor
"""

# Create a new tensor with default datatype (float32)
B = tf.constant([1.7, 7.4])
B.dtype

C = tf.constant([7, 10])
C.dtype

# Change from float32 to float16 (reduced precision)
D = tf.cast(B, dtype = tf.float16)
D

# Change from int32 to float32
E = tf.cast(C, dtype=tf.float32)
E

E_float16 = tf.cast(E, dtype=tf.float16)
E_float16.dtype

"""### Aggregating tensors

Aggregating tensors = condensing them from multiple values down to a smaller amount of values.
"""

# Get the absolute value
D = tf.constant([-7, -10])
D

# Get the absolute values
tf.abs(D)

"""Let's go through the following forms of aggregation:
* Get the minimum
* Get the maximum
* Get the mean of a tensor
* Get the sum of a tensor
"""

# Create a random tensor with values between 0 and 100 of size 50
E = tf.constant(np.random.randint(0, 100, size=50))
E

tf.size(E), E.shape, E.ndim

# Find the minimum
tf.reduce_min(E)

# Find the maximum
tf.reduce_max(E)

# Find the mean of a tensor
tf.reduce_mean(E)

# Find the sum of a tensor
tf.reduce_sum(E)

# Find the variance of a tensor, 2 farklı yolu var, ayrıca variance ve std icin tensor tipi real or complex olmak zorunda ornek float32
E = tf.cast(E, dtype=tf.float32)
tf.math.reduce_variance(E)

import tensorflow_probability as tfp
tfp.stats.variance(E)

# Find the standard deviation of a tensor
tf.math.reduce_std(E)

"""### Find the positional minimum and maximum

Positional min and max genellikle output kisminda en yuksek ya da en dusuk ihtimali olan degerleri ararken ise yarar.
"""

# Create a new tensor for finding positional minimum and maximum
tf.random.set_seed(42)
F = tf.random.uniform(shape=[50])
F

# Find the positional maximum
tf.argmax(F)

# Index on our largest value position
F[tf.argmax(F)]

# Find the max value of F
tf.reduce_max(F)

# Check for equality
F[tf.argmax(F)] == tf.reduce_max(F)

# Find the positional minimum
tf.argmin(F)

# Find the minimum using the positional minimum index
F[tf.argmin(F)]

"""### Squeezing a tensor (removing all single dimensions)"""

# Create a tensor to get started
tf.random.set_seed(42)
G = tf.constant(tf.random.uniform(shape=[50]), shape=(1, 1, 1, 1, 50))
G

G.shape

G_squeezed = tf.squeeze(G)
G_squeezed, G_squeezed.shape

"""### One-hot encoding tensors

One-hot encoding is a form of numerical encoding
"""

# Create a list of indices
some_list = [0, 1, 2, 3] # could be red, green, blue, purple

# One-hot encode our list of indices
tf.one_hot(some_list, depth=4)

# Specify custom values for one-hot encoding
tf.one_hot(some_list, depth=4, on_value="I love Deep Learning", off_value="I also like to dance")

"""### Squaring, log, square root"""

# Create a new tensor
H = tf.range(1, 10)
H

# Square it
tf.square(H)

# Find the square root(will give error, method requires non-int type)
tf.sqrt(H)

# Find the square root
tf.sqrt(tf.cast(H, dtype=tf.float32))

# Find the log
tf.math.log(H)

# Find the log
tf.math.log(tf.cast(H, dtype=tf.float32))

"""### Tensors and NumPy

TensorFlow interacts beautifully with NumPy arrays.

**Note:** One of the main differences between a TensorFlow tensor and a NumPy array is that a TensorFlow tensor can be run on a GPU or TPU
(for faster numerical processing).
"""

# Create a tensor directly from a NumPy array
J = tf.constant(np.array([3., 7., 10.]))
J

# Convert our tensor back to a NumPy array
np.array(J), type(np.array(J))

# Convert tensor J to a NumPy array
J.numpy(), type(J.numpy())

# Tensorflow ile NumPy arasinda gecis yapabilmenin bu kadar kolay olmasinin en iyi ozelligi sorun ciktiginda birbirine cevirip array ile oynayabiliriz.
# Cok fazla imkan taniyor ve isimizi kolaylastiriyor.
J = tf.constant([3.])
J.numpy()[0]

# The default types of each are slightly different
numpy_J = tf.constant(np.array([3., 7., 10.]))
tensor_J = tf.constant([3., 7., 10.])
# Check the datatypes of each
numpy_J.dtype, tensor_J.dtype

"""### Finding access to GPUs"""

import tensorflow as tf
tf.config.list_physical_devices()

!nvidia-smi

"""**Note:** If you have access to a CUDA-enabled GPU, TensorFlow will automatically use it whenever possible."""

